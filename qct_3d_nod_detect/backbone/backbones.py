# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/backbone/14_backbones.ipynb.

# %% auto 0
__all__ = ['logger', 'DropPath', 'LayerScale', 'TubePatchEmbed3D', 'get_3d_sincos_pos_embed', 'MLP', 'Attention', 'Block',
           'ViT3D', 'load_state_dict_into_vit', 'build_vit_backbone_with_fpn', 'MultiModalAtlas']

# %% ../../nbs/backbone/14_backbones.ipynb 0
from typing import Tuple, Dict, Literal, Union, List
import torch
import torch.nn as nn
import torch.nn.functional as F
from pathlib import Path
from loguru import logger
import math
from ..fpn import BackboneFPN, SimpleFPN, AtlasAdapter, BackboneAtlasFPN
import yaml

class DropPath(nn.Module):
    def __init__(self, drop_prob: float = 0.0) -> None:
        super().__init__()
        self.drop_prob = drop_prob

    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]
        if self.drop_prob == 0.0 or not self.training:
            return x
        keep_prob = 1 - self.drop_prob
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)
        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
        random_tensor.floor_()
        return x.div(keep_prob) * random_tensor


class LayerScale(nn.Module):
    def __init__(self, dim: int, init_values: float = 1e-5) -> None:
        super().__init__()
        self.gamma = nn.Parameter(init_values * torch.ones(dim))

    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]
        return x * self.gamma


class TubePatchEmbed3D(nn.Module):
    def __init__(
        self,
        img_size: Tuple[int, int, int],
        patch_size: Tuple[int, int, int],
        in_channels: int,
        embed_dim: int,
    ) -> None:
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.grid_size = (
            img_size[0] // patch_size[0],
            img_size[1] // patch_size[1],
            img_size[2] // patch_size[2],
        )
        self.num_patches = self.grid_size[0] * self.grid_size[1] * self.grid_size[2]
        self.proj = nn.Conv3d(
            in_channels,
            embed_dim,
            kernel_size=patch_size,
            stride=patch_size,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]
        x = self.proj(x)
        b, d, z, y, xdim = x.shape
        return x.permute(0, 2, 3, 4, 1).reshape(b, z * y * xdim, d)


def get_3d_sincos_pos_embed(embed_dim: int, grid_size: Tuple[int, int, int]) -> torch.Tensor:
    gz, gy, gx = grid_size
    grid_z = torch.arange(gz, dtype=torch.float32)
    grid_y = torch.arange(gy, dtype=torch.float32)
    grid_x = torch.arange(gx, dtype=torch.float32)
    grid = torch.stack(torch.meshgrid(grid_z, grid_y, grid_x, indexing="ij"), dim=-1).reshape(-1, 3)

    def _emb(dim_val: int, coord: torch.Tensor) -> torch.Tensor:
        omega = torch.arange(dim_val // 2, dtype=torch.float32)
        if len(omega) > 1:
            omega = omega / (len(omega) - 1)
        omega = 1.0 / (10000**omega)
        out = torch.einsum("n,d->nd", coord, omega)
        return torch.cat([out.sin(), out.cos()], dim=1)

    dim_z = embed_dim // 3
    dim_y = embed_dim // 3
    dim_x = embed_dim - dim_z - dim_y
    pos = torch.cat(
        [
            _emb(dim_z, grid[:, 0]),
            _emb(dim_y, grid[:, 1]),
            _emb(dim_x, grid[:, 2]),
        ],
        dim=1,
    )
    if pos.shape[1] < embed_dim:
        pad = embed_dim - pos.shape[1]
        pos = torch.nn.functional.pad(pos, (0, pad))
    return pos


class MLP(nn.Module):
    def __init__(self, dim: int, mlp_ratio: float, drop: float) -> None:
        super().__init__()
        hidden = int(dim * mlp_ratio)
        self.fc1 = nn.Linear(dim, hidden)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(hidden, dim)
        self.drop = nn.Dropout(drop)

    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class Attention(nn.Module):
    def __init__(
        self,
        dim: int,
        num_heads: int,
        qkv_bias: bool,
        attn_drop: float,
        proj_drop: float,
        use_sdpa: bool,
    ) -> None:
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.scale = self.head_dim**-0.5
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        self.use_sdpa = use_sdpa and hasattr(F, "scaled_dot_product_attention")

    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]
        b, n, c = x.shape
        qkv = self.qkv(x).reshape(b, n, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        if self.use_sdpa:
            x = F.scaled_dot_product_attention(q, k, v)
        else:
            attn = (q @ k.transpose(-2, -1)) * self.scale
            attn = attn.softmax(dim=-1)
            attn = self.attn_drop(attn)
            x = attn @ v
        x = x.transpose(1, 2).reshape(b, n, c)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class Block(nn.Module):
    def __init__(
        self,
        dim: int,
        num_heads: int,
        mlp_ratio: float = 4.0,
        qkv_bias: bool = True,
        drop: float = 0.0,
        attn_drop: float = 0.0,
        drop_path: float = 0.0,
        use_layer_scale: bool = True,
        layer_scale_init: float = 1e-5,
        use_sdpa: bool = True,
    ) -> None:
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = Attention(dim, num_heads, qkv_bias, attn_drop, drop, use_sdpa)
        self.drop_path = DropPath(drop_path) if drop_path > 0 else nn.Identity()
        self.norm2 = nn.LayerNorm(dim)
        self.mlp = MLP(dim, mlp_ratio, drop)
        self.use_layer_scale = use_layer_scale
        if use_layer_scale:
            self.ls1 = LayerScale(dim, layer_scale_init)
            self.ls2 = LayerScale(dim, layer_scale_init)

    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]
        if self.use_layer_scale:
            x = x + self.drop_path(self.ls1(self.attn(self.norm1(x))))
            x = x + self.drop_path(self.ls2(self.mlp(self.norm2(x))))
        else:
            x = x + self.drop_path(self.attn(self.norm1(x)))
            x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x


class ViT3D(nn.Module):
    def __init__(
        self,
        img_size: Tuple[int, int, int],
        patch_size: Tuple[int, int, int],
        in_channels: int,
        embed_dim: int,
        depth: int,
        num_heads: int,
        mlp_ratio: float,
        qkv_bias: bool,
        drop_rate: float,
        attn_drop_rate: float,
        drop_path_rate: float,
        use_cls_token: bool,
        use_sdpa: bool,
        use_layer_scale: bool = True,
        layer_scale_init: float = 1e-5,
        num_register_tokens: int = 0,
    ) -> None:
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.embed_dim = embed_dim
        self.use_cls_token = use_cls_token
        self.num_register_tokens = num_register_tokens

        self.patch_embed = TubePatchEmbed3D(img_size, patch_size, in_channels, embed_dim)
        num_patches = self.patch_embed.num_patches
        self.prefix_tokens = (1 if use_cls_token else 0) + num_register_tokens
        self.grid_size = (
            img_size[0] // patch_size[0],
            img_size[1] // patch_size[1],
            img_size[2] // patch_size[2],
        )

        if use_cls_token:
            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        if num_register_tokens > 0:
            self.register_tokens = nn.Parameter(torch.zeros(1, num_register_tokens, embed_dim))

        pos_embed = get_3d_sincos_pos_embed(embed_dim, self.grid_size)
        seq = []
        if use_cls_token:
            seq.append(torch.zeros(1, embed_dim))
        if num_register_tokens > 0:
            seq.append(torch.zeros(num_register_tokens, embed_dim))
        seq.append(pos_embed)
        full_pos = torch.cat(seq, dim=0).unsqueeze(0)
        self.pos_embed = nn.Parameter(full_pos, requires_grad=False)
        self.pos_drop = nn.Dropout(p=drop_rate)

        dpr = torch.linspace(0, drop_path_rate, depth).tolist()
        self.blocks = nn.ModuleList(
            [
                Block(
                    dim=embed_dim,
                    num_heads=num_heads,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=qkv_bias,
                    drop=drop_rate,
                    attn_drop=attn_drop_rate,
                    drop_path=dpr[i],
                    use_layer_scale=use_layer_scale,
                    layer_scale_init=layer_scale_init,
                    use_sdpa=use_sdpa,
                )
                for i in range(depth)
            ]
        )
        self.norm = nn.LayerNorm(embed_dim)
        self.output_dim = embed_dim
        self.num_patches = num_patches

    def forward(self, x: torch.Tensor) -> dict[str, torch.Tensor]:  # type: ignore[override]
        b = x.shape[0]
        tokens = self.patch_embed(x)  # [B, N, D]
        if self.use_cls_token:
            cls_tokens = self.cls_token.expand(b, -1, -1)
            tokens = torch.cat([cls_tokens, tokens], dim=1)
        if self.num_register_tokens > 0:
            regs = self.register_tokens.expand(b, -1, -1)
            tokens = (
                torch.cat([tokens[:, :1, :], regs, tokens[:, 1:, :]], dim=1)
                if self.use_cls_token
                else torch.cat([regs, tokens], dim=1)
            )
        tokens = tokens + self.pos_embed.to(tokens.dtype).to(tokens.device)
        tokens = self.pos_drop(tokens)
        for blk in self.blocks:
            tokens = blk(tokens)
        tokens = self.norm(tokens)
        patch_tokens = tokens[:, self.prefix_tokens :, :]
        global_token = tokens[:, 0] if self.use_cls_token else patch_tokens.mean(dim=1)
        return {
            "feat_tokens": tokens,
            "patch_tokens": patch_tokens,
            "global": global_token,
        }

# %% ../../nbs/backbone/14_backbones.ipynb 1
def _clean_key(key: str) -> str:
    for prefix in ("model.", "module."):
        if key.startswith(prefix):
            key = key[len(prefix) :]
    if key.startswith("encoder."):
        key = key[len("encoder.") :]
    return key


def _adapt_patch_embed_channels(
    weight: torch.Tensor, target_shape: torch.Size, in_chans: int
) -> torch.Tensor | None:
    if weight.ndim != 5 or weight.shape[2:] != target_shape[2:]:
        return None
    out_channels, old_in, _, _, _ = weight.shape
    if out_channels != target_shape[0]:
        return None
    if old_in == in_chans:
        return weight
    if in_chans == 1:
        return weight.mean(dim=1, keepdim=True)
    if in_chans < old_in:
        trimmed = weight[:, :in_chans, :, :, :]
        return trimmed * (old_in / in_chans)
    repeat = math.ceil(in_chans / old_in)
    expanded = weight.repeat(1, repeat, 1, 1, 1)[:, :in_chans, :, :, :]
    expanded *= old_in / in_chans
    return expanded

def load_state_dict_into_vit(vit: ViT3D, ckpt_path: Path) -> tuple[list[str], list[str]]:

    raw = torch.load(str(ckpt_path), map_location="cpu", weights_only=False)
    sd = raw.get("state_dict", raw.get("model", raw))
    if not isinstance(sd, dict):
        if hasattr(sd, "state_dict"):
            sd = sd.state_dict()
        else:
            raise RuntimeError("Unsupported checkpoint format.")
    target = vit.state_dict()
    cleaned: Dict[str, torch.Tensor] = {}
    for k, v in sd.items():
        key = _clean_key(k)
        if key == "pos_embed" and v.shape != target.get("pos_embed", v).shape:
            continue
        if (
            key == "patch_embed.proj.weight"
            and key in target
            and v.shape[1] != target[key].shape[1]
        ):
            adapted = _adapt_patch_embed_channels(v, target[key].shape, target[key].shape[1])
            if adapted is not None:
                cleaned[key] = adapted
                continue
        if key in target and getattr(v, "shape", None) == target[key].shape:
            cleaned[key] = v
    missing, unexpected = vit.load_state_dict(cleaned, strict=False)
    if missing:
        logger.info("Missing MAE keys during load: %s", missing)
    if unexpected:
        logger.info("Unexpected MAE keys during load: %s", unexpected)
    return list(missing), list(unexpected)

import torch

def load_clean_pt(model, pt_path, strict=False, map_location="cpu"):

    """
    Load a cleaned .pt state_dict into a model and report missing/unexpected keys.
    """
    
    state_dict = torch.load(pt_path, map_location=map_location)

    missing, unexpected = model.load_state_dict(state_dict, strict=strict)

    print(f"Loaded checkpoint: {pt_path}")
    print(f"Strict mode      : {strict}")
    print(f"Missing keys     : {len(missing)}")
    print(f"Unexpected keys  : {len(unexpected)}")

    if missing:
        print("\nMissing keys (sample):")
        for k in missing[:20]:
            print("  ", k)

    if unexpected:
        print("\nUnexpected keys (sample):")
        for k in unexpected[:20]:
            print("  ", k)

    return missing, unexpected

def build_vit_backbone_with_fpn(
    variant: Literal["S", "L"],
    ckpt_path: Union[Path, str],
    scales: Tuple[float, ...] = (1, 2, 0.5, 0.25),
    out_channels: int = 256,
) -> nn.Module:

    if variant == "S":
        model_config = {
            "img_size": (128, 128, 128),
            "patch_size": (16, 16, 16),
            "in_channels": 1,
            "embed_dim": 384,
            "depth": 12,
            "num_heads": 6,
            "mlp_ratio": 4.0,
            "qkv_bias": True,
            "drop_rate": 0.0,
            "attn_drop_rate": 0.0,
            "drop_path_rate": 0.1,
            "use_cls_token": True,
            "use_sdpa": True, # TODO Populated with sample, will change later
        }

    elif variant == "L":
        model_config = {
            "img_size": (128, 128, 128),
            "patch_size": (8, 8, 8),
            "in_channels": 1,
            "embed_dim": 1024,
            "depth": 24,
            "num_heads": 16,
            "mlp_ratio": 4.0,
            "qkv_bias": True,
            "drop_rate": 0.0,
            "attn_drop_rate": 0.0,
            "drop_path_rate": 0.0,
            "use_cls_token": True,
            "use_sdpa": True,
        }


    # Load backbone
    backbone = ViT3D(**model_config)
    if ckpt_path is not None:
        if not isinstance(ckpt_path, Path):
            ckpt_path = Path(ckpt_path)
            print(f"Loading from pretrained checkpoint - {ckpt_path}")

        missing, unexpected = load_state_dict_into_vit(backbone, ckpt_path)
        print(f"Missing keys: {missing}, Unexpected keys: {unexpected}")
    
    else:
        print("loading random weights")

    # Build with FPN
    fpn = SimpleFPN(dim=model_config["embed_dim"], out_channels=out_channels, scales=scales)
    backbone_fpn = BackboneFPN(
        backbone=backbone,
        fpn=fpn,
        patch_grid_size=(
            model_config["img_size"][0] // model_config["patch_size"][0],
            model_config["img_size"][1] // model_config["patch_size"][1],
            model_config["img_size"][2] // model_config["patch_size"][2],
        ),
    )

    return backbone_fpn

# %% ../../nbs/backbone/14_backbones.ipynb 3
import logging
import math
from einops import rearrange
from torch import Tensor

from qct_3d_nod_detect.backbone.atlas_encoders import (
    AbdomenCTEmbed3D,
    BMRMultiConvEmbed3D,
    BrainCTMultiConvEmbed3D,
    ChestCT488Embed3D,
    ChestCTEmbed3D,
    ConvCXREmbed,
    MultiConvEmbed3D,
    MultiViewConvCXR,
    ProstateMRMultiConvEmbed3D,
)

from .multimodal_msa import AtlasStage

logger = logging.getLogger(__name__)

class MultiModalAtlas(nn.Module):
    def __init__(
        self,
        args,
        kwargs_list=None,
        embed_dim=384,
        num_classes=100,
        fc_norm=nn.LayerNorm,
        multiscale_feats=True,
        model_config={},
        verbose=False,
        modalities={"single_view": {"num_views": 1, "view_pos_embed": None}},
        **kwargs,
    ):
        """
        Args:
            args: full argument config
            kwargs_list : generate different scales
            embed_dim : embedding dimension
            num_classes : number of classes
            fc_norm : normalization layer
            multiscale_feats : whether to use multiscale features
            modalities : dictionary of modalities
                - single_view_2d
                    - num_views : number of views
                    - view_pos_embed : view-specific positional embeddings
                - multi_view_2d
                    - num_views : number of views
                    - view_pos_embed : view-specific positional embeddings
                    - view_pos_embed_type : type of view-specific positional embeddings
                - single_view_3d
                    - num_views
        """

        super().__init__()
        # assert kwargs_list is not None, "kwargs_list should be provided"
        self.args = args
        # self.kwargs_list = kwargs_list
        self.num_classes = num_classes
        self.image_size = 256
        self.multiscale_feats = multiscale_feats
        self.embed_dim = embed_dim
        self.verbose = verbose
        self.model_config = model_config
        assert self.model_config is not None, "model_config should be provided"

        self._init_patch_embeds()
        self._init_atlas_stages()
        self._init_pools()

    def set_grad_checkpointing(self, enable=True):
        for stage in self.atlas_models:
            stage.set_grad_checkpointing(enable)

    def build_scales(self, x, batch=None):
        """
        Expects input to be dictionary of tensors,
        """
        modality, image = list(x.items())[0]
        modal_config = self.model_config["modalities"]

        if self.verbose:
            print(f"Image shape before patch embed: {image.shape}")

        patch_embed_fn = self.patch_embeds[modality]
        patch_embed_xBCDHW = patch_embed_fn(image)

        if self.verbose:
            print(f"Image shape after patch embed: {patch_embed_xBCDHW.shape}")

        multiscale_feats, grid_sizes = self._build_multiscale_tokens(
            patch_embed_xBCDHW,
            merge_ratio=modal_config[modality]["merge_ratio"],
            local2global=modal_config[modality]["local2global"],
        )
        return multiscale_feats, grid_sizes

    def _build_multiscale_tokens(
            self, 
            x_BCDHW, 
            merge_ratio, 
            local2global)->Tuple[List[Tensor], List[List[int]]]:
        seqlen = x_BCDHW.shape[2] * x_BCDHW.shape[3] * x_BCDHW.shape[4]
        downsampling_ratio = local2global[0] * local2global[1] * local2global[2]
        kernel_size = (1, 1, 1)

        min_seqlen = merge_ratio[0] * merge_ratio[1] * merge_ratio[2] # merge_ratio = window_size

        num_stages = math.ceil((math.log2(seqlen / min_seqlen) / math.log2(downsampling_ratio))) + 1

        if self.verbose:    
            print(f"Num stages - {num_stages}, seqlen: {seqlen}, min_seqlen: {min_seqlen}, downsampling_ratio: {downsampling_ratio}")

        stages = []
        grid_sizes = []

        for scale in range(num_stages):
            local2global_op = nn.MaxPool3d(kernel_size=kernel_size)
            x_scale_BCDHW = local2global_op(x_BCDHW)
            if self.verbose:
                print(f"Scale {scale}: x_BCDHW shape: {x_BCDHW.shape}")
                print(f"Scale {scale}: kernel_size: {kernel_size}, x_scale_BCDHW shape: {x_scale_BCDHW.shape}")
                
            kernel_size = tuple(k * l for k, l in zip(kernel_size, local2global))

            # check if windowing is possible
            b, c, d, h, w = x_scale_BCDHW.shape

            ## if exact merge is possible, then dont add another scale
            if d == merge_ratio[0] and h == merge_ratio[1] and w == merge_ratio[2]:
                x_scale_win = rearrange(x_scale_BCDHW, "b c d h w -> b (d h w) c")
                grid_size = [1, 1, 1]

            ## only reduce in dimensions where merge is possible
            elif d >= merge_ratio[0] and h >= merge_ratio[1] and w >= merge_ratio[2]:
                # run windowing
                x_scale_win = rearrange(
                    x_scale_BCDHW,
                    "b c (d m0) (h m1) (w m2) -> b d h w m0 m1 m2 c",
                    m0=merge_ratio[0],
                    m1=merge_ratio[1],
                    m2=merge_ratio[2],
                )
                grid_size = [
                    x_scale_win.shape[2],
                    x_scale_win.shape[3],
                    x_scale_win.shape[1],
                ]
                x_scale_win = rearrange(x_scale_win, "b d h w m0 m1 m2 c -> (b d h w) (m0 m1 m2) c")
            else:
                x_scale_win = rearrange(x_scale_BCDHW, "b c d h w -> b (d h w) c")
                grid_size = [1, 1, 1]

            stages.append(x_scale_win)
            grid_sizes.append(grid_size)

            if math.prod(grid_size) == 1:
                break

        return stages, grid_sizes

    def prepare_multiscale_layout(self, img_size, merge_ratio, local2global, patch_size):
        """
        given the input size, merge_ratio and local2global
        prepare the layout for multiscale attention and config
        for the architecture
        """
        H, W, D = img_size
        pH, pW, pD = patch_size
        lD, lH, lW = local2global
        mD, mH, mW = merge_ratio

        h, w, d = H // pH, W // pW, D // pD
        seqlen = h * w * d

        downsampling_ratio = lD * lH * lW
        min_seqlen = mD * mH * mW

        kD, kH, kW = 1, 1, 1

        if seqlen <= min_seqlen:
            return [
                {
                    "grid_size": [1, 1, 1],
                    "window_size": seqlen,
                    "num_windows": 1,
                    "seq_length": seqlen,
                    "window_dims": [h, w, d],
                }
            ]
        num_stages = math.ceil((math.log2(seqlen / min_seqlen) / math.log2(downsampling_ratio))) + 1

        multiscale_layout = []

        for scale in range(num_stages):
            h0, w0, d0 = h // kH, w // kW, d // kD

            # if h0 <= mH or w0 <= mW or d0 <= mD:
            if h0 == mH and w0 == mW and d0 == mD:
                # if exact merge is possible, then dont add another scale
                multiscale_layout.append(
                    {
                        "grid_size": [1, 1, 1],
                        "window_size": h0 * w0 * d0,
                        "num_windows": 1,
                        "seq_length": h0 * w0 * d0,
                        "window_dims": [h0, w0, d0],
                    }
                )
                break
            elif h0 >= mH and w0 >= mW and d0 >= mD:
                grid_size = [h0 // mH, w0 // mW, d0 // mD]
                multiscale_layout.append(
                    {
                        "grid_size": grid_size,
                        "window_size": mH * mW * mD,
                        "num_windows": grid_size[0] * grid_size[1] * grid_size[2],
                        "seq_length": h0 * w0 * d0,
                        "window_dims": [mH, mW, mD],
                    }
                )
            else:
                multiscale_layout.append(
                    {
                        "grid_size": [1, 1, 1],
                        "window_size": h0 * w0 * d0,
                        "num_windows": 1,
                        "seq_length": h0 * w0 * d0,
                        "window_dims": [h0, w0, d0],
                    }
                )
                break

            kD *= lD
            kH *= lH
            kW *= lW
        return multiscale_layout

    def forward(self, x, batch=None, *args, **kwargs):

        modality, image = list(x.items())[0]
        # modality = 'chest_ct'
        # image = x
        bsz = image.shape[0]
        logger.debug(f"Entering MultiModalAtlas forward with modality: {modality}, batch size: {bsz}")
        x, grid_sizes = self.build_scales(x, batch)

        modal_config = self.model_config["modalities"][modality]
        image_size = modal_config["image_size"]
        merge_ratio = modal_config["merge_ratio"]
        local2global = modal_config["local2global"]
        patch_size = modal_config["patch_size"]
        multiscale_layout = self.prepare_multiscale_layout(
            img_size=image_size,
            merge_ratio=merge_ratio,
            local2global=local2global,
            patch_size=patch_size,
        )

        grid_sizes = [layout["grid_size"] for layout in multiscale_layout]

        multiscale_feats = []

        for level, atlas_model in enumerate(self.atlas_models):
            is_last = len(x) == 1 or level == len(self.atlas_models) - 1
            x = atlas_model(
                x,
                grid_sizes=grid_sizes[level:],
                multiscale_layout=multiscale_layout[level:],
                merge_ratio=merge_ratio,
                local2global=local2global,
                modality=modality,
            )
            if not is_last:
                multiscale_feats.append(x[0])
                x = x[1:]

        multiscale_feats.append(x[0])

        feats_for_head = []
        for i, scale_tokens in enumerate(multiscale_feats):
            # Need to know if scale_tokens is windowed (B*NW, K, C) or flattened (B, N, C)
            # Assuming it's (B*NW, K, C) if NW > 1 based on layout
            if math.prod(grid_sizes[i]) > 1 and len(scale_tokens.shape) == 3:  # Check if likely windowed format
                rearranged_scale = rearrange(scale_tokens, "(b nw) k c -> b (nw k) c", b=bsz)
                logger.debug(f"  Rearranging scale {i} (windowed) {scale_tokens.shape} -> {rearranged_scale.shape}")
                feats_for_head.append(rearranged_scale)
            elif len(scale_tokens.shape) == 3 and scale_tokens.shape[0] == bsz:  # Already (B, N, C)
                logger.debug(f"  Using scale {i} (already BNC) shape: {scale_tokens.shape}")
                feats_for_head.append(scale_tokens)
            else:
                logger.warning(f"  Unexpected shape for scale {i}: {scale_tokens.shape}. Attempting BNC rearrange.")
                # Try a generic rearrange, might fail if batch dim isn't divisible
                try:
                    rearranged_scale = rearrange(scale_tokens, "(b nw) k c -> b (nw k) c", b=bsz)
                    feats_for_head.append(rearranged_scale)
                except Exception as e:
                    logger.error(f"    Failed to rearrange scale {i}: {e}. Skipping this scale for readout.")
                    continue

        if self.multiscale_feats:
            return feats_for_head[0] # The scale 1 from stage 2
        else:
            ## return maxpool on last scale features only
            x = feats_for_head[0]
            x = self.maxpool(x.transpose(1, 2)).squeeze(2)
        return x

    def _init_patch_embeds(self):

        modalities = self.model_config["modalities"]
        MODALITY_TO_PATCH_EMBED = {
            "chest_xray_single_view": ConvCXREmbed,
            "chest_xray_two_view": MultiViewConvCXR,
            "chest_ct": ChestCTEmbed3D,
            "abdomen_ct": ChestCTEmbed3D,
            "breast_mr": BMRMultiConvEmbed3D,
            "brain_ct": BrainCTMultiConvEmbed3D,
            "prostate_mr": ProstateMRMultiConvEmbed3D,
        }

        self.patch_embeds = nn.ModuleDict()
        for modality in modalities:

            image_size = modalities[modality].get("image_size", self.image_size)
            patch_size = modalities[modality].get("patch_size", 16)
            in_chans = modalities[modality].get("in_chans", 1)
            num_features = modalities[modality].get("num_features", 192)

            logging.info(
                f"Initializing patch embed for modality: {modality}, image_size: {image_size}, patch_size: {patch_size}, in_chans: {in_chans}, num_features: {num_features}"
            )

            self.patch_embeds[modality] = MODALITY_TO_PATCH_EMBED[modality](
                vol_size=image_size,
                patch_size=patch_size,
                in_chans=in_chans,
                embed_dim=num_features,
            )

    def _init_atlas_stages(self):

        atlas_models = []
        atlas_args = self.model_config["model"]["atlas_args"]
        num_scales = len(self.model_config["model"]["stages"])
        for stage_idx, depth in enumerate(self.model_config["model"]["stages"]):
            atlas_args["depth"] = depth
            atlas_args["num_scales"] = num_scales
            atlas_models.append(AtlasStage(**atlas_args))

        self.atlas_models = nn.ModuleList(atlas_models)

    def _init_pools(self):
        self.maxpool = nn.AdaptiveMaxPool1d(1)

def build_atlas_backbone_with_fpn(
    config_path: Union[Path, str],
    scales: Tuple[float, ...] = (1, 2, 0.5, 0.25),
    feat_channels: int = 384,
    out_channels: int = 256,
    ckpt_path: str = None,
) -> nn.Module:

    with open(config_path, "r") as file:
        model_config = yaml.safe_load(file)

    encoder = MultiModalAtlas(
        args=model_config["vision_cfg"],
        embed_dim=model_config["embed_dim"],
        num_classes=2,
        model_config=model_config["vision_cfg"]["model_config"],
    )

    if ckpt_path:
        print(f"Loading from checkpoint - {ckpt_path}")
        load_clean_pt(encoder, ckpt_path)
    else:
        print("Loading random weights")

    adapter = AtlasAdapter(
        in_dim=feat_channels, 
        out_dim=out_channels,
    )

    fpn = SimpleFPN(
        dim=out_channels,
        out_channels=out_channels,
        scales=scales,
    )

    backbone_fpn = BackboneAtlasFPN(
        backbone=encoder,
        adapter=adapter,
        fpn=fpn,
    )

    return backbone_fpn