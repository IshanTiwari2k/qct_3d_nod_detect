# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/roi/16_box_heads.ipynb.

# %% auto 0
__all__ = ['get_norm', 'FastRCNNConvFCHead3D']

# %% ../../nbs/roi/16_box_heads.ipynb 0
from torch import nn
from typing import List
import numpy as np
from ..layers import ShapeSpec

import torch.nn as nn

def get_norm(norm, num_channels, dim=3):
    """
    Args:
        norm (str or callable or None):
            - "" or None: no normalization
            - "BN": BatchNorm
            - "GN": GroupNorm (32 groups)
            - callable: custom norm layer
        num_channels (int): number of channels
        dim (int): 2 or 3 (Conv2d / Conv3d)
    """
    if not norm:
        return None

    if callable(norm):
        return norm(num_channels)

    norm = norm.upper()

    if norm == "BN":
        return nn.BatchNorm3d(num_channels) if dim == 3 else nn.BatchNorm2d(num_channels)

    if norm == "GN":
        return nn.GroupNorm(32, num_channels)

    if norm == "LN":
        return nn.GroupNorm(1, num_channels)

    raise ValueError(f"Unsupported norm type: {norm}")

class FastRCNNConvFCHead3D(nn.Sequential):
    """
    3D version of FastRCNNConvFCHead.
    Consists of Conv3D layers followed by FC layers.
    """

    def __init__(
        self,
        input_shape: ShapeSpec,
        *,
        conv_dims: List[int],
        fc_dims: List[int],
        conv_norm: str = "",
    ):
        super().__init__()
        assert len(conv_dims) + len(fc_dims) > 0

        # input_shape: (C, D, H, W)
        self._output_size = (
            input_shape.channels,
            input_shape.depth,
            input_shape.height,
            input_shape.width,
        )

        # -------------------------
        # Conv3D stack
        # -------------------------
        self.conv_norm_relus = []
        for k, conv_dim in enumerate(conv_dims):
            conv = nn.Conv3d(
                in_channels=self._output_size[0],
                out_channels=conv_dim,
                kernel_size=3,
                padding=1,
                bias=not conv_norm,
            )

            self.add_module(f"conv{k+1}", conv)
            if conv_norm:
                self.add_module(
                    f"conv{k+1}_norm",
                    get_norm(conv_norm, conv_dim),
                )
            self.add_module(f"conv{k+1}_relu", nn.ReLU(inplace=True))

            self.conv_norm_relus.append(conv)
            self._output_size = (
                conv_dim,
                self._output_size[1],
                self._output_size[2],
                self._output_size[3],
            )

        # -------------------------
        # FC stack
        # -------------------------
        self.fcs = []
        for k, fc_dim in enumerate(fc_dims):
            if k == 0:
                self.add_module("flatten", nn.Flatten(start_dim=1))

            fc = nn.Linear(int(np.prod(self._output_size)), fc_dim)
            self.add_module(f"fc{k+1}", fc)
            self.add_module(f"fc{k+1}_relu", nn.ReLU(inplace=True))

            self.fcs.append(fc)
            self._output_size = fc_dim

        # -------------------------
        # Initialization
        # -------------------------
        for layer in self.conv_norm_relus:
            nn.init.kaiming_normal_(layer.weight, mode="fan_out", nonlinearity="relu")
        for layer in self.fcs:
            nn.init.xavier_uniform_(layer.weight)
            nn.init.constant_(layer.bias, 0)

    @classmethod
    def from_config(cls, cfg, input_shape):
        num_conv = cfg.MODEL.ROI_BOX_HEAD.NUM_CONV
        conv_dim = cfg.MODEL.ROI_BOX_HEAD.CONV_DIM
        num_fc = cfg.MODEL.ROI_BOX_HEAD.NUM_FC
        fc_dim = cfg.MODEL.ROI_BOX_HEAD.FC_DIM

        return {
            "input_shape": input_shape,
            "conv_dims": [conv_dim] * num_conv,
            "fc_dims": [fc_dim] * num_fc,
            "conv_norm": cfg.MODEL.ROI_BOX_HEAD.NORM,
        }

    def forward(self, x):
        for layer in self:
            x = layer(x)
        return x

    @property
    def output_shape(self):
        o = self._output_size
        if isinstance(o, int):
            return ShapeSpec(channels=o)
        else:
            return ShapeSpec(
                channels=o[0],
                depth=o[1],
                height=o[2],
                width=o[3],
            )
