# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/fpn/13_SimpleFPN.ipynb.

# %% auto 0
__all__ = ['SimpleFPN', 'BackboneFPN']

# %% ../../nbs/fpn/13_SimpleFPN.ipynb 1
from torch import nn
import torch
from typing import Tuple, Dict

class AtlasAdapter(nn.Module):
    """
    Adapts Atlas patch tokens to a dense 32³ feature map
    using ConvTranspose3d (learned upsampling).

    Input:
        patch_tokens: (B, 8192, 384)  where 8192 = 32*16*16
    Output:
        feat_map: (B, C, 32, 32, 32)
    """

    def __init__(
        self,
        in_dim: int = 384,
        out_dim: int = 256,
        patch_grid_size: Tuple[int, int, int] = (32, 16, 16),
    ):
        super().__init__()
        self.patch_grid_size = patch_grid_size

        # project token dim → channel dim
        self.proj = nn.Conv3d(in_dim, out_dim, kernel_size=1)
        self.norm = nn.GroupNorm(1, out_dim)

        # upsample H,W: 16 → 32 (D already 32)
        self.upsample_hw = nn.ConvTranspose3d(
            out_dim,
            out_dim,
            kernel_size=(1, 2, 2),
            stride=(1, 2, 2),
        )

    def forward(self, patch_tokens: torch.Tensor) -> torch.Tensor:
        B, N, C = patch_tokens.shape
        Dp, Hp, Wp = self.patch_grid_size
        assert N == Dp * Hp * Wp, f"Expected {Dp*Hp*Wp}, got {N}"

        x = (
            patch_tokens
            .view(B, Dp, Hp, Wp, C)
            .permute(0, 4, 1, 2, 3)
            .contiguous()
        )  # (B, C, 32, 16, 16)

        x = self.proj(x)
        x = self.norm(x)

        # learned upsampling → (B, C, 32, 32, 32)
        x = self.upsample_hw(x)

        return x

class SimpleFPN(nn.Module):
    def __init__(self, dim, out_channels, scales):
        super().__init__()
        self.blocks = nn.ModuleDict()

        scale_to_level = {
            2: "p2",
            1: "p3",
            0.5: "p4",
            0.25: "p5",
        }

        for scale in sorted(scales, reverse=True):
            layers = []

            if scale == 2:
                layers.append(nn.ConvTranspose3d(dim, dim, 2, 2))
            elif scale == 1:
                pass
            elif scale == 0.5:
                layers.append(nn.Conv3d(dim, dim, 2, 2))
            elif scale == 0.25:
                layers.append(nn.Conv3d(dim, dim, 2, 2))
                layers.append(nn.Conv3d(dim, dim, 2, 2))
            else:
                raise NotImplementedError

            layers.extend([
                nn.Conv3d(dim, out_channels, 1),
                nn.GroupNorm(1, out_channels),
                nn.Conv3d(out_channels, out_channels, 3, padding=1),
                nn.GroupNorm(1, out_channels),
            ])

            self.blocks[scale_to_level[scale]] = nn.Sequential(*layers)

    def forward(self, x):
        return {k: block(x) for k, block in self.blocks.items()}

class BackboneFPN(nn.Module):

    def __init__(
        self,
        backbone: nn.Module,
        fpn: nn.Module,
        patch_grid_size: Tuple[int, int, int] = (16, 16, 16),
    ):
        super().__init__()
        self.backbone = backbone
        self.fpn = fpn
        self.patch_grid_size = patch_grid_size

        block = fpn.blocks["p2"]

        for layer in reversed(block):
            if isinstance(layer, nn.Conv3d):
                self.out_channels = layer.out_channels
                break
            
        if self.out_channels is None:
            raise ValueError("Could not determine out_channels from FPN block.")

        self.out_channels = fpn.blocks["p2"][-1].num_channels \
            if isinstance(fpn.blocks["p2"][-1], nn.GroupNorm) \
                else None
        
    def forward(self, x):

        """
        Args:
            x: Tensor[B, 1, D, H, W]
        Returns:
            Dict[str, Tensor]: multi-scale features
        """

        feat = self.backbone(x)
        patch_tokens = feat['patch_tokens']  # B, N, C
        B, N, C = patch_tokens.shape

        Dp, Hp, Wp = self.patch_grid_size
        assert N == Dp * Hp * Wp, f"Expected N={Dp*Hp*Wp}, got N={N}"
        feat = (patch_tokens.view(B, Dp, Hp, Wp, C).permute(0, 4, 1, 2, 3).contiguous())
        features = self.fpn(feat)

        return features

class BackboneAtlasFPN(nn.Module):
    """
    Atlas backbone + ConvTranspose adapter + SimpleFPN
    """

    def __init__(
        self,
        backbone: nn.Module,
        fpn: SimpleFPN,
        adapter: AtlasAdapter,
    ):
        super().__init__()
        self.backbone = backbone
        self.adapter = adapter
        self.fpn = fpn
        self.out_channels = next(iter(fpn.blocks.values()))[-1].num_channels

    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        patch_tokens = self.backbone(x)

        dense_feat = self.adapter(patch_tokens)  # (B, C, 32, 32, 32)
        pyramid = self.fpn(dense_feat)

        return pyramid