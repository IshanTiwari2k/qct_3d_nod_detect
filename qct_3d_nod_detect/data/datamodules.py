# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/data/lightning_datamodule.ipynb.

# %% auto 0
__all__ = ['detection_collate', 'DetDataModule']

# %% ../../nbs/data/lightning_datamodule.ipynb 0
from typing import List, Dict, Optional
import torch
from .configs import DetDatasetConfig

def detection_collate(batch: List[Dict]):

    images, targets = [], []

    C = None
    for b in batch:
        if b != "invalid":
            C = b["image"].shape[0]
            break
    if C is None:
        C = 1  # fallback if *all* samples are invalid

    for b in batch:
        if b == "invalid":
            img = torch.zeros(
                (C, 128, 128, 128),
                dtype=torch.float32,
            )
            tgt = {
                "boxes": torch.empty((0, 6), dtype=torch.float32),
                "labels": torch.empty((0,), dtype=torch.int64),
            }
        else:
            img = b["image"]
            tgt = {
                "boxes": b["gt_boxes"],
                "labels": b["gt_classes"],
            }

        images.append(img)
        targets.append(tgt)

    images = torch.stack(images, dim=0)  # [B, C, D, H, W]

    return images, targets

# %% ../../nbs/data/lightning_datamodule.ipynb 1
import lightning.pytorch as pl
from torch.utils.data import DataLoader
from . import DetDataset

class DetDataModule(pl.LightningDataModule):
    def __init__(
        self,
        cfg,
        num_workers: int = 4,
        pin_memory: bool = True,
        persistent_workers: bool = True,
    ):
        """
        Args:
            cfg: DetDatasetConfig
            batch_size: batch size per GPU
            num_workers: dataloader workers
            pin_memory: pin memory for CUDA
            persistent_workers: keep workers alive
        """
        super().__init__()

        self.cfg = cfg
        self.batch_size = cfg.batch_size
        self.num_workers = num_workers
        self.pin_memory = pin_memory
        self.persistent_workers = persistent_workers

    def setup(self, stage: Optional[str] = None):

        if stage in (None, "fit", "train"):

            print(f"patch size - {self.cfg.patch_size}")

            train_ds_cfg = DetDatasetConfig(
                cache_root_path=self.cfg.cache_root_path,
                windows=self.cfg.windows,
                centers_to_exclude=self.cfg.centers_to_exclude,
                annotated_by=self.cfg.annotated_by,
                scan_annot_type=self.cfg.scan_annot_type,
                patch_size=self.cfg.patch_size,
                split="train",
            )

            self.train_dataset = DetDataset(train_ds_cfg)

        if stage in (None, "fit", "validate"):

            val_ds_cfg = DetDatasetConfig(
                cache_root_path=self.cfg.cache_root_path,
                windows=self.cfg.windows,
                centers_to_exclude=self.cfg.centers_to_exclude,
                annotated_by=self.cfg.annotated_by,
                scan_annot_type=self.cfg.scan_annot_type,
                patch_size=self.cfg.patch_size,
                split="val",
            )

            self.val_dataset = DetDataset(val_ds_cfg)

    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            persistent_workers=self.persistent_workers,
            collate_fn=detection_collate,
        )

    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            persistent_workers=self.persistent_workers,
            collate_fn=detection_collate,
        )

    def test_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            persistent_workers=self.persistent_workers,
            collate_fn=detection_collate,
        )
