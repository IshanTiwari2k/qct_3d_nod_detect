# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/training/21_lightning.ipynb.

# %% auto 0
__all__ = ['FasterRCNN3DLightning']

# %% ../../nbs/training/21_lightning.ipynb 0
from . import BaseLightningModule
from torch import nn
from ..metrics import BoxMetrics3D
from ..structures import Instances3D, Boxes3D
import torch
from typing import List

class FasterRCNN3DLightning(BaseLightningModule):
    def __init__(
            self,
            model: nn.Module,
            learning_rate: float = 1e-4,
            grad_clip_val: float = 0.0,
            grad_clip_algorithm: str = "norm",
            log_on: str = "step",
            modules_to_freeze: List[str] = [],
    ):
        
        super().__init__(learning_rate=learning_rate)

        self.model = model
        self.grad_clip_val = grad_clip_val
        self.grad_clip_algorithm = grad_clip_algorithm
        self.log_on = log_on

        self.box_metrics = BoxMetrics3D(
            iou_thresholds=[0.1, 0.2, 0.3, 0.4, 0.5]
        )

        self.train_predictions, self.val_predictions = [], []

        self.train_metrics_batches = 10
        self.freeze_by_name(modules_to_freeze) # TODO make it configurable

    def _build_targets(
            self,
            images,
            targets
    ):
        
        image_sizes = images.shape[-3:]
        instances = []

        for tgt in targets:
            inst = Instances3D(image_size=image_sizes)

            inst.gt_boxes = Boxes3D(tgt['boxes'])
            inst.gt_classes = tgt['labels'].long()

            instances.append(inst)

        return instances
    
    def forward(
            self,
            images
    ):
        return self.model.forward_inference(images)
    
    def training_step(self, batch, batch_idx):

        images, targets = batch
        targets = self._build_targets(images, targets)

        out = self.model.forward_train(images, targets)
        loss_dict, stat_dict = out['losses'], out['stats']
        total_loss = sum(loss_dict.values())

        self.log_dict(
            {f"train/losses/{k}": v for k, v in loss_dict.items()},
            prog_bar=False,
            sync_dist=True
        )

        self.log_dict(
            {f"train/{k}": v for k, v in stat_dict.items()},
            prog_bar=True,
            sync_dist=True,
        )

        self.log(
            "train/loss_total",
            total_loss,
            prog_bar=True,
            sync_dist=True,
        )

        if batch_idx%self.train_metrics_batches==0:

            with torch.no_grad():
                proposals = self.model.forward_inference(images)

            for prop, tgt in zip(proposals, targets):

                if len(prop) == 0:
                    pred_boxes = torch.empty((0, 6), device="cpu")
                    pred_scores = torch.empty((0,), device="cpu")
                else:
                    scores = torch.sigmoid(prop.objectness_logits).detach().cpu()
                    pred_boxes = prop.proposal_boxes.tensor.detach().cpu()
                    pred_scores = scores.detach().cpu()

                gt_boxes = tgt.gt_boxes.tensor.detach().cpu()

                self.train_predictions.append({
                    "pred_boxes": pred_boxes.numpy(),
                    "pred_scores": pred_scores.numpy(),
                    "gt_boxes": gt_boxes.numpy(),
                })

        return total_loss
    
    def on_train_epoch_end(self):

        if len(self.train_predictions) == 0:
            return

        metrics = self.box_metrics(self.train_predictions)

        self.log("train/metrics/mAP", metrics["mAP"], sync_dist=True)
        self.log("train/metrics/mAR", metrics["mAR"], sync_dist=True)

        for iou, ap in metrics["AP"].items():
            self.log(f"train/metrics/AP@{iou}", ap, sync_dist=True)

        for iou, ar in metrics["AR"].items():
            self.log(f"train/metrics/AR@{iou}", ar, sync_dist=True)

        self.train_predictions.clear()

    @torch.no_grad()
    def validation_step(
            self, 
            batch, 
            batch_idx
        ):

        images, targets = batch
        targets = self._build_targets(images, targets)

        out = self.model.forward_train(images, targets)
        loss_dict, stat_dict = out['losses'], out['stats']

        self.log_dict(
            {f"val/losses/{k}": v for k, v in loss_dict.items()},
            prog_bar=False,
            sync_dist=True
        )

        self.log_dict(
            {f"val/{k}": v for k, v in stat_dict.items()},
            prog_bar=True,
            sync_dist=True,
        )

        proposals = self.model.forward_inference(images)

        for prop, tgt in zip(proposals, targets):

            if len(prop) == 0:
                pred_boxes = torch.empty((0, 6), device="cpu")
                pred_scores = torch.empty((0,), device="cpu")
            else:
                scores = torch.sigmoid(prop.objectness_logits).detach().cpu()
                pred_boxes = prop.proposal_boxes.tensor.detach().cpu()
                pred_scores = scores.detach().cpu()

            gt_boxes = tgt.gt_boxes.tensor.detach().cpu()

            self.val_predictions.append({
                "pred_boxes": pred_boxes.numpy(),
                "pred_scores": pred_scores.numpy(),
                "gt_boxes": gt_boxes.numpy(),
            })

        return None
    
    def on_validation_epoch_end(self):

        if len(self.val_predictions) == 0:
            return

        metrics = self.box_metrics(self.val_predictions)

        self.log("val/metrics/mAP", metrics["mAP"], prog_bar=True, sync_dist=True)
        self.log("val/metrics/mAR", metrics["mAR"], prog_bar=True, sync_dist=True)

        for iou, ap in metrics["AP"].items():
            self.log(f"val/metrics/AP@{iou}", ap, sync_dist=True)

        for iou, ar in metrics["AR"].items():
            self.log(f"val/metrics/AR@{iou}", ar, sync_dist=True)

        self.val_predictions.clear()

    def freeze_by_name(
        self,
        freeze_patterns: list[str],
        verbose: bool = True,
    ):
        """
            Freeze parameters whose names contain any of `freeze_patterns`.
            Everything else remains trainable.
        """

        frozen = []

        for name, param in self.model.named_parameters():
            if any(pat in name for pat in freeze_patterns):
                param.requires_grad = False
                frozen.append(name)

        if verbose:
            print("=== Freezing summary ===")
            print(f"Frozen params: {len(frozen)}")

            for n in frozen[:20]:
                print(" ", n)
            if len(frozen) > 20:
                print(f"  ... +{len(frozen) - 20} more")

        return frozen

    def configure_optimizers(self):
        return torch.optim.AdamW(
            self.model.parameters(),
            lr=self.learning_rate,
            weight_decay=1e-4,
        )
