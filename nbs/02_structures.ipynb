{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "501a3d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp structures\n",
    "#| export\n",
    "import torch\n",
    "from typing import Tuple, List, Dict, Any, Union, Optional\n",
    "from torch import device\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "class Boxes3D:\n",
    "\n",
    "    def __init__(self, tensor: torch.Tensor):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor[Float]): a Nx6 matrix where each row is (x1, y1, z1, x2, y2, z2)\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(tensor, torch.Tensor):\n",
    "            tensor = torch.as_tensor(tensor, dtype=torch.float32, device=torch.device(\"cpu\"))\n",
    "        else:\n",
    "            tensor = tensor.to(torch.float32)\n",
    "\n",
    "        if tensor.numel() == 0:\n",
    "            tensor = tensor.reshape((-1, 4)).to(dtype=torch.float32)\n",
    "\n",
    "        assert tensor.dim() == 2 and tensor.size(-1) == 6, tensor.size()\n",
    "\n",
    "        self.tensor = tensor\n",
    "    \n",
    "    def clone(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Clone the boxes\n",
    "\n",
    "        Returns: \n",
    "            Boxes3D\n",
    "        \"\"\"\n",
    "\n",
    "        return Boxes3D(self.tensor.clone())\n",
    "\n",
    "    def to(self, device: torch.device):\n",
    "        return Boxes3D(self.tensor.to(device=device))\n",
    "\n",
    "    def volume(self):\n",
    "        \"\"\"\n",
    "        Computes the area of all the boxes\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: a vector with the area of each box\n",
    "        \"\"\"\n",
    "\n",
    "        box = self.tensor\n",
    "        volume = (box[:, 3] - box[:, 0]) * (box[:, 4] - box[:, 1]) * (box[:, 5] - box[:, 2])\n",
    "        return volume\n",
    "    \n",
    "    def clip(self, box_size: Tuple[int, int]) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Clip (in place) the boxes by limiting x coordinates to the range [0, width]\n",
    "        and y coordinates in the range [0, height].git/\n",
    "\n",
    "        Args:\n",
    "            box_size (height, width): The clipping box's size\n",
    "        \"\"\"\n",
    "\n",
    "        assert torch.isfinite(self.tensor).all(), \"Box tensor contains infinite or Nan\"\n",
    "\n",
    "        h, w, d = box_size\n",
    "        x1 = self.tensor[:, 0].clamp(min=0, max=w)\n",
    "        y1 = self.tensor[:, 1].clamp(min=0, max=h)\n",
    "        z1 = self.tensor[:, 2].clamp(min=0, max=d)\n",
    "\n",
    "        x2 = self.tensor[:, 3].clamp(min=0, max=w)\n",
    "        y2 = self.tensor[:, 4].clamp(min=0, max=h)\n",
    "        z2 = self.tensor[:, 5].clamp(min=0, max=d)\n",
    "\n",
    "        self.tensor = torch.stack((x1, y1, z1, x2, y2, z2), dim=-1)\n",
    "\n",
    "    def nonempty(self, threshold, float=0.0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Find boxes that are non-empty.\n",
    "        A box is considered empty, if either of its side is no larger than threshold.\n",
    "\n",
    "        Returns:\n",
    "            Tensor:\n",
    "                a binary vector which represents whether each box is empty\n",
    "                (False) or non-empty (True).\n",
    "        \"\"\"\n",
    "\n",
    "        box = self.tensor\n",
    "        widths = box[:, 2] - box[:, 0]\n",
    "        heights = box[:, 3] - box[:, 1]\n",
    "        keep = (widths > threshold) & (heights > threshold)\n",
    "        return keep\n",
    "    \n",
    "    def __getitem__(self, item) -> \"Boxes3D\":\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            item: int, slice, or a BoolTensor\n",
    "\n",
    "        Returns:\n",
    "            Boxes3D: Create a new :class:`Boxes3D` by indexing.\n",
    "\n",
    "        The following usage are allowed:\n",
    "\n",
    "        1. `new_boxes = boxes[3]`: return a `Boxes3D` which contains only one box.\n",
    "        2. `new_boxes = boxes[2:10]`: return a slice of boxes.\n",
    "        3. `new_boxes = boxes[vector]`, where vector is a torch.BoolTensor\n",
    "           with `length = len(boxes)`. Nonzero elements in the vector will be selected.\n",
    "\n",
    "        Note that the returned Boxes might share storage with this Boxes,\n",
    "        subject to Pytorch's indexing semantics.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(item, int):\n",
    "            return Boxes3D(self.tensor[item].view(1, -1))\n",
    "        \n",
    "        b = self.tensor[item]\n",
    "        assert b.dim() == 2, \"Indexing on Boxes3D with {} failed to return a matrix!\".format(item)\n",
    "        return Boxes3D(b)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensor.shape[0]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Boxes3D(\" + str(self.tensor) + \")\"\n",
    "    \n",
    "    def inside_box(self, box_size: Tuple[int, int], boundary_threshold: int = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            box_size (height, width): Size of the reference box.\n",
    "            boundary_threshold (int): Boxes that extend beyond the reference box\n",
    "                boundary by more than boundary_threshold are considered \"outside\".\n",
    "\n",
    "        Returns:\n",
    "            a binary vector, indicating whether each box is inside the reference box.\n",
    "        \"\"\"\n",
    "\n",
    "        height, width, depth = box_size\n",
    "        inds_inside = (\n",
    "            (self.tensor[..., 0] >= -boundary_threshold)\n",
    "            & (self.tensor[..., 1] >= -boundary_threshold)\n",
    "            & (self.tensor[..., 2] >= -boundary_threshold)\n",
    "            & (self.tensor[..., 3] < width + boundary_threshold)\n",
    "            & (self.tensor[..., 4] < height + boundary_threshold) \n",
    "            & (self.tensor[..., 5] < depth + boundary_threshold) \n",
    "        )\n",
    "\n",
    "        return inds_inside\n",
    "        \n",
    "    def get_centers(self) -> torch.Tensor:\n",
    "\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            the box center in Nx3 array of shape (x, y, z)\n",
    "        \"\"\"\n",
    "\n",
    "        return (self.tensor[:, :3] + self.tensor[:, 3:])/2\n",
    "\n",
    "    def scale(self, scale_x: float, scale_y: float, scale_z: float) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Scales the box with horizontal, vertical and depth scaling factors\n",
    "        \"\"\"\n",
    "\n",
    "        self.tensor[:, 0::3] *= scale_x # x1, x2\n",
    "        self.tensor[:, 1::3] *= scale_y # y1, y2\n",
    "        self.tensor[:, 2::3] *= scale_z # z1, z2\n",
    "\n",
    "    @classmethod\n",
    "    def cat(cls, boxes_list: List[\"Boxes3D\"]) -> \"Boxes3D\":\n",
    "\n",
    "        \"\"\"\n",
    "        Concatenates a list of Boxes3D into a single Boxes3D\n",
    "        \"\"\"\n",
    "        assert isinstance(boxes_list, (list, tuple))\n",
    "        if len(boxes_list) == 0:\n",
    "            return cls(torch.empty(0))\n",
    "\n",
    "        assert all([isinstance(box, Boxes3D) for box in boxes_list])\n",
    "\n",
    "        cat_boxes = cls(torch.cat([b.tensor for b in boxes_list], dim=0))\n",
    "        return cat_boxes\n",
    "\n",
    "    @property\n",
    "    def device(self) -> device:\n",
    "        return self.tensor.device\n",
    "\n",
    "    def __iter__(self):\n",
    "        yield from self.tensor\n",
    "\n",
    "\n",
    "class Instances3D:\n",
    "\n",
    "    def __init__(self, image_size: Tuple[int, int], **kwargs):\n",
    "\n",
    "        self._image_size = image_size\n",
    "        self._fields: Dict[str, Any] = {}\n",
    "\n",
    "        for k, v in kwargs.items():\n",
    "            self.set(k, v)\n",
    "\n",
    "    @property\n",
    "    def image_size(self) -> Tuple[int, int]:\n",
    "        return self._image_size\n",
    "\n",
    "    def __setattr__(self, name: str, value: Any) -> None:\n",
    "        if name.startswith(\"_\"):\n",
    "            super().__setattr__(name, value)\n",
    "        else:\n",
    "            self.set(name, value)\n",
    "\n",
    "    def __getattr__(self, name: str) -> Any:\n",
    "        if name == \"_fields\" or name not in self._fields:\n",
    "            raise AttributeError(\"Cannot find field '{}' in the given Instances!\".format(name))\n",
    "        return self._fields[name]\n",
    "\n",
    "    def set(self, name: str, value: Any) -> None:\n",
    "\n",
    "        with warnings.catch_warnings(record=True):\n",
    "            data_len = len(value)\n",
    "        if len(self._fields):\n",
    "            assert (\n",
    "                len(self) == data_len\n",
    "            ), \"Adding a field of length {} to a Instances of length {}\".format(data_len, len(self))\n",
    "        self._fields[name] = value\n",
    "\n",
    "    def has(self, name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            bool: whether the field called name exists\n",
    "        \"\"\"\n",
    "\n",
    "        return name in self._fields\n",
    "    \n",
    "    def clone(self) -> \"Instances3D\":\n",
    "\n",
    "        \"\"\"\n",
    "        Deep copy of Instances3D.\n",
    "        Matches detectron2 Instances.clone() semantics.\n",
    "        \"\"\"\n",
    "\n",
    "        new = Instances3D(self._image_size)\n",
    "        for k, v in self._fields.items():\n",
    "            if hasattr(v, \"clone\"):\n",
    "                new._fields[k] = v.clone()\n",
    "            else:\n",
    "                new._fields[k] = v\n",
    "\n",
    "        return new\n",
    "\n",
    "    def remove(self, name: str) -> None:\n",
    "        del self._fields[name]\n",
    "\n",
    "    def get(self, name: str) -> Any:\n",
    "        return self._fields[name]\n",
    "\n",
    "    def get_fields(self) -> Dict[str, Any]:\n",
    "        return self._fields\n",
    "\n",
    "    def to(self, *args: Any, **kwargs: Any) -> \"Instances3D\":\n",
    "        ret = Instances3D(self._image_size)\n",
    "        for k, v in self._fields.items():\n",
    "            if hasattr(v, \"to\"):\n",
    "                v = v.to(*args, **kwargs)\n",
    "            ret.set(k, v)\n",
    "        return ret\n",
    "\n",
    "    def __getitem__(self, item: Union[int, slice, torch.BoolTensor]) -> \"Instances3D\":\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            item: an index-like object and will be used to index all the fields.\n",
    "\n",
    "        Returns:\n",
    "            If `item` is a string, return the data in the corresponding field.\n",
    "            Otherwise, returns an `Instances` where all fields are indexed by `item`.\n",
    "        \"\"\"\n",
    "\n",
    "        if type(item) is int:\n",
    "            if item >= len(self) or item < -len(self):\n",
    "                raise IndexError(\"Instances index out of range\")\n",
    "            else:\n",
    "                item = slice(item, None, len(self))\n",
    "\n",
    "        ret = Instances3D(self._image_size)\n",
    "        for k, v in self._fields.items():\n",
    "            ret.set(k, v[item])\n",
    "        return ret\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        for v in self._fields.values():\n",
    "            return v.__len__()\n",
    "\n",
    "        raise NotImplementedError(\"Empty Instances does not supprt __len__\")\n",
    "    \n",
    "    def __iter__(self):\n",
    "        raise NotImplementedError(\"Instances3D is not iterable\")\n",
    "\n",
    "    @staticmethod\n",
    "    def cat(instances_lists: List[\"Instances3D\"]) -> \"Instances3D\":\n",
    "\n",
    "        assert all(isinstance(i, Instances3D) for i in instances_lists)\n",
    "        assert len(instances_lists) > 0\n",
    "\n",
    "        if len(instances_lists) == 1:\n",
    "            return instances_lists[0]\n",
    "\n",
    "        image_size = instances_lists[0].image_size\n",
    "        if not isinstance(image_size, torch.Tensor):\n",
    "            for i in instances_lists[1:]:\n",
    "                assert i.image_size == image_size\n",
    "\n",
    "        ret = Instances3D(image_size)\n",
    "        for k in instances_lists[0]._fields.keys():\n",
    "            values = [i.get(k) for i in instances_lists]\n",
    "            v0 = values[0]\n",
    "            if isinstance(v0, torch.Tensor):\n",
    "                values = torch.cat(values, dim=0)\n",
    "            elif isinstance(v0, list):\n",
    "                values = list(itertools.chain(*values))\n",
    "            elif hasattr(type(v0), \"cat\"):\n",
    "                values = type(v0).cat(values)\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported type {} for concatentation\".format(type(v0)))\n",
    "            ret.set(k, values)\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        s = self.__class__.__name__ + \"(\"\n",
    "        s += \"num_instances={}, \".format(len(self))\n",
    "        s += \"image_height={}, \".format(self._image_size[0])\n",
    "        s += \"image_width={}, \".format(self._image_size[1])\n",
    "        s += \"fields=[{}])\".format(\", \".join((f\"{k}: {v}\" for k, v in self._fields.items())))\n",
    "        return s\n",
    "\n",
    "    __repr__ = __str__\n",
    "\n",
    "def pairwise_intersection(boxes1: Boxes3D, boxes2: Boxes3D) -> torch.Tensor:\n",
    "\n",
    "    boxes1, boxes2 = boxes1.tensor, boxes2.tensor\n",
    "    depth_height_width = (\n",
    "        torch.min(boxes1[:, None, 3:], boxes2[:, 3:]) - \n",
    "        torch.max(boxes1[:, None, :3], boxes2[:, :3])\n",
    "    ) # (N, M, 3)\n",
    "\n",
    "    depth_height_width.clamp_(min=0)\n",
    "\n",
    "    intersection = depth_height_width.prod(dim=2) \n",
    "    return intersection\n",
    "\n",
    "def pairwise_iou_3d(boxes1: Boxes3D, boxes2: Boxes3D) -> torch.Tensor:\n",
    "\n",
    "    \"\"\"\n",
    "    Given two lists of boxes of sizes N and M computes the IoU \n",
    "    (intersection over Union) between **all** N x M pairs of boxes.\n",
    "    The box order must be (xmin, ymin, xmax, ymax).\n",
    "    \"\"\"\n",
    "\n",
    "    vol1 = boxes1.volume()\n",
    "    vol2 = boxes2.volume()\n",
    "\n",
    "    inter = pairwise_intersection(boxes1, boxes2)\n",
    "\n",
    "    iou = torch.where(\n",
    "        inter > 0,\n",
    "        inter / (vol1[:, None] + vol2 - inter),\n",
    "        torch.zeros(1, dtype=inter.dtype, device=inter.device)\n",
    "    )\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67efe4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ImagesList3D:\n",
    "    \"\"\"\n",
    "    Structure that holds a list of 3D images (volumes) of possibly\n",
    "    varying sizes as a single padded tensor.\n",
    "\n",
    "    Attributes:\n",
    "        tensor (Tensor): shape (N, C, D, H, W)\n",
    "        image_sizes (list[tuple[int, int, int]]): original (D, H, W) for each image\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            tensor: torch.Tensor,\n",
    "            image_sizes: List[Tuple[int, int, int]]\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): shape (N, C, D, H, W)\n",
    "            image_sizes (list[(D, H, W)]): original sizes (before padding)\n",
    "        \"\"\"\n",
    "\n",
    "        self.tensor = tensor\n",
    "        self.image_sizes = image_sizes\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_sizes)\n",
    "\n",
    "    def __getitem__(self, idx) -> torch.Tensor:\n",
    "\n",
    "        \"\"\"\n",
    "        Access individual volume in its original size.\n",
    "        \"\"\"\n",
    "\n",
    "        d, h, w = self.image_sizes[idx]\n",
    "        return self.tensor[idx, ..., :d, :h, :w]\n",
    "    \n",
    "    def to(self, *args: Any, **kwargs: Any) -> \"ImagesList3D\":\n",
    "        return ImagesList3D(self.tensor.to(*args, **kwargs), self.image_sizes)\n",
    "\n",
    "    @property\n",
    "    def device(self) -> device:\n",
    "        return self.tensor.device\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_tensors(\n",
    "        tensors: List[torch.Tensor],\n",
    "        size_divisibility: int = 0,\n",
    "        pad_value: float = 0.0,\n",
    "        padding_constraints: Optional[Dict[str, int]] = None,\n",
    "    ) -> \"ImagesList3D\":\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensors: list of Tensors of shape (C, D, H, W)\n",
    "            size_divisibility: if > 0, pad D/H/W to be divisible by this\n",
    "            pad_value: padding value\n",
    "\n",
    "        Returns:\n",
    "            ImageList3D\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(tensors) > 0\n",
    "        assert isinstance(tensors, (list, tuple))\n",
    "\n",
    "        for t in tensors:\n",
    "            assert t.dim() == 4, f\"Expected (C, D, H, W), got {t.shape}\"\n",
    "            assert t.shape[:-3] == tensors[0].shape[:-3]\n",
    "\n",
    "        image_sizes = [(t.shape[-3], t.shape[-2], t.shape[-1]) for t in tensors]\n",
    "\n",
    "        max_d = max(s[0] for s in image_sizes)\n",
    "        max_h = max(s[1] for s in image_sizes)\n",
    "        max_w = max(s[2] for s in image_sizes)\n",
    "\n",
    "        # Handle padding constraints\n",
    "        if padding_constraints is not None:\n",
    "            cube_size = padding_constraints.get(\"cube_size\", 0)\n",
    "            if cube_size > 0:\n",
    "                max_d = max_h = max_w = cube_size\n",
    "\n",
    "            if \"size_divisibility\" in padding_constraints:\n",
    "                size_divisibility = padding_constraints[\"size_divisibility\"]\n",
    "\n",
    "        if size_divisibility > 1:\n",
    "            def _ceil(x, d): return ((x + d - 1) // d)*d\n",
    "\n",
    "            max_d = _ceil(max_d, size_divisibility)\n",
    "            max_h = _ceil(max_h, size_divisibility)\n",
    "            max_w = _ceil(max_w, size_divisibility)\n",
    "        \n",
    "        batch_shape = (\n",
    "            len(tensors),\n",
    "            tensors[0].shape[0],\n",
    "            max_d,\n",
    "            max_h,\n",
    "            max_w\n",
    "        )\n",
    "\n",
    "        batched = tensors[0].new_full(batch_shape, pad_value)\n",
    "\n",
    "        for i, vol in enumerate(tensors):\n",
    "            d, h, w = vol.shape[-3:]\n",
    "            batched[i, :, :d, :h, :w].copy_(vol)\n",
    "\n",
    "        return ImagesList3D(batched.contiguous(), image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f41c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qct_nod_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
