{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a606b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp memory\n",
    "#| export\n",
    "\n",
    "import torch\n",
    "from functools import wraps\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def _ignore_torch_cuda_oom():\n",
    "    \"\"\"\n",
    "    A context which ignores CUDA OOM exception from pytorch.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        yield\n",
    "    except RuntimeError as e:\n",
    "        # NOTE: the string may change?\n",
    "        if \"CUDA out of memory. \" in str(e):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def retry_if_cuda_oom(func):\n",
    "\n",
    "    \"\"\"\n",
    "    Makes a function retry itself after encountering\n",
    "    pytorch's CUDA OOM error.\n",
    "    It will first retry after calling `torch.cuda.empty_cache()`.\n",
    "\n",
    "    If that still fails, it will then retry by trying to convert inputs to CPUs.\n",
    "    In this case, it expects the function to dispatch to CPU implementation.\n",
    "    The return values may become CPU tensors as well and it's user's\n",
    "    responsibility to convert it back to CUDA tensor if needed.\n",
    "\n",
    "    Args:\n",
    "        func: a stateless callable that takes tensor-like objects as arguments\n",
    "\n",
    "    Returns:\n",
    "        a callable which retries `func` if OOM is encountered.\n",
    "\n",
    "    Examples:\n",
    "    ::\n",
    "        output = retry_if_cuda_oom(some_torch_function)(input1, input2)\n",
    "        # output may be on CPU even if inputs are on GPU\n",
    "\n",
    "    Note:\n",
    "        1. When converting inputs to CPU, it will only look at each argument and check\n",
    "           if it has `.device` and `.to` for conversion. Nested structures of tensors\n",
    "           are not supported.\n",
    "\n",
    "        2. Since the function might be called more than once, it has to be\n",
    "           stateless.\n",
    "    \"\"\"\n",
    "\n",
    "    def maybe_to_cpu(x):\n",
    "        try:\n",
    "            like_gpu_tensor = x.device.type==\"cuda\" and hasattr(x, \"to\")\n",
    "        except AttributeError:\n",
    "            like_gpu_tensor = False\n",
    "        if like_gpu_tensor:\n",
    "            return x.to(device='cpu')\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    @wraps(func)\n",
    "    def wrapped(*args, **kwargs):\n",
    "\n",
    "        with _ignore_torch_cuda_oom():\n",
    "            return func(*args, **kwargs)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        with _ignore_torch_cuda_oom():\n",
    "            return func(*args, **kwargs)\n",
    "\n",
    "        # Try on CPU. This slows down the code significantly, therefore print a notice.\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.info(\"Attempting to copy inputs of {} to CPU due to CUDA OOM\".format(str(func)))\n",
    "        new_args = (maybe_to_cpu(x) for x in args)\n",
    "        new_kwargs = {k: maybe_to_cpu(v) for k, v in kwargs.items()}\n",
    "        return func(*new_args, **new_kwargs)\n",
    "\n",
    "    return wrapped"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
