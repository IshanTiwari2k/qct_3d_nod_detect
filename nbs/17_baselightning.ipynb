{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5313edcf",
   "metadata": {},
   "source": [
    "## Base Lightning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab7bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp base_lightning\n",
    "#| export\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Dict, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2a5a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseLightningModule(pl.LightningModule):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: 1e-3\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x\n",
    "    ):\n",
    "\n",
    "        raise NotImplementedError(\"Implement the forward function\")\n",
    "\n",
    "    def clip_gradients(\n",
    "        self,\n",
    "        optimizer: Optional[torch.optim.Optimizer] = None,\n",
    "        gradient_clip_val: Optional[float] = None,\n",
    "        gradient_clip_algorithm: Optional[str] = None,\n",
    "    ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "            Clip / normalize gradients. Call this **after** .backward() and **before** .step().\n",
    "            \n",
    "            Supports:\n",
    "            - norm-based clipping    (default: global norm)\n",
    "            - value-based clipping   (element-wise hard clip)\n",
    "            - set grad_clip_val=0 or None → no clipping\n",
    "            \n",
    "            Works safely even with mixed precision / gradient accumulation.\n",
    "        \"\"\"\n",
    "\n",
    "        clip_val = self.grad_clip_val\n",
    "        clip_algo = self.grad_clip_algorithm\n",
    "\n",
    "        if clip_val is None or clip_val <= 0:\n",
    "            return\n",
    "\n",
    "        if optimizer is None:\n",
    "            parameters = self.parameters()\n",
    "        else:\n",
    "            parameters = [p for group in optimizer.param_groups for p in group[\"params\"]\n",
    "                          if p.grad is not None]\n",
    "            \n",
    "        if not parameters:\n",
    "            return\n",
    "\n",
    "        if clip_algo == \"value\":\n",
    "            torch.nn.utils.clip_grad_value_(parameters, clip_val)\n",
    "        else:\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                parameters,\n",
    "                max_norm=clip_val,\n",
    "                norm_type=2.0, # L2\n",
    "            )\n",
    "\n",
    "            self.log(\"grad_norm\", grad_norm, on_step=True, on_epoch=False, sync_dist=True)\n",
    "\n",
    "    def log_dict_helper(\n",
    "        self,\n",
    "        metrics: Dict[str, Union[torch.Tensor, float, int]],\n",
    "        prefix: str = \"\",\n",
    "    ) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Logs a dictionary of metrics with smart control over step/epoch logging.\n",
    "        \n",
    "        - Automatically adds prefix (e.g. \"train/\", \"val/\")\n",
    "        - Respects self.log_on = \"step\" | \"epoch\" | \"both\"\n",
    "        - Handles tensors → scalars automatically\n",
    "        - commit=None → Lightning decides (usually True on epoch end)\n",
    "        \"\"\"\n",
    "\n",
    "        if not metrics:\n",
    "            return\n",
    "\n",
    "        on_step = self.log_on in (\"step\", \"both\")\n",
    "        on_epoch = self.log_on in (\"epoch\", \"both\")\n",
    "\n",
    "        logged = {}\n",
    "\n",
    "        for k, v in metrics.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "\n",
    "                v = v.detach()\n",
    "                if v.numel() == 1:\n",
    "                    v = v.item()\n",
    "\n",
    "                else:\n",
    "                    v = v.mean().item()\n",
    "\n",
    "            key = f\"{prefix}{k}\" if prefix else k\n",
    "            self.log(\n",
    "                key,\n",
    "                v,\n",
    "                on_step=on_step,\n",
    "                on_epoch=on_epoch,\n",
    "                prog_bar=(\"loss\" in k.lower() or \"acc\" in k.lower()),\n",
    "                sync_dist=True,\n",
    "            )\n",
    "\n",
    "            logged[key] = v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qct_nod_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
