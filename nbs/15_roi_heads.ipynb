{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "389291f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp roi_heads\n",
    "#| export\n",
    "from torch import nn\n",
    "import torch\n",
    "from qct_3d_nod_detect.structures import Instances3D, pairwise_iou_3d\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import math\n",
    "\n",
    "def add_ground_truth_to_proposals_3d(\n",
    "    targets: List[Instances3D],\n",
    "    proposals: List[Instances3D],\n",
    ") -> List[Instances3D]:\n",
    "    \"\"\"\n",
    "    Augment proposals with ground-truth boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(targets) == len(proposals)\n",
    "\n",
    "    new_proposals = []\n",
    "\n",
    "    for proposals_per_image, targets_per_image in zip(proposals, targets):\n",
    "\n",
    "        if len(targets_per_image) == 0:\n",
    "            new_proposals.append(proposals_per_image)\n",
    "            continue\n",
    "\n",
    "        # Clone to avoid in-place modification\n",
    "        proposals_per_image = proposals_per_image.clone()\n",
    "\n",
    "        gt_boxes = targets_per_image.gt_boxes\n",
    "        device = gt_boxes.tensor.device\n",
    "\n",
    "        # Create new Instances3D for GT boxes\n",
    "        gt_proposals = Instances3D(proposals_per_image.image_size)\n",
    "        gt_proposals.proposal_boxes = gt_boxes\n",
    "\n",
    "        # Objectness logits: high confidence for GT\n",
    "        gt_logit_value = math.log((1.0 - 1e-10) / (1 - (1.0 - 1e-10)))\n",
    "        gt_logits = gt_logit_value * torch.ones(len(gt_boxes), device=device)\n",
    "        gt_proposals.objectness_logits = gt_logits\n",
    "\n",
    "        # Concatenate proposals\n",
    "        proposals_per_image = Instances3D.cat(\n",
    "            [proposals_per_image, gt_proposals]\n",
    "        )\n",
    "\n",
    "        new_proposals.append(proposals_per_image)\n",
    "\n",
    "    return new_proposals\n",
    "\n",
    "def subsample_labels(\n",
    "    labels: torch.Tensor,\n",
    "    num_samples: int,\n",
    "    positive_fraction: float,\n",
    "    num_classes: int,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        labels (Tensor): shape (N,), values in:\n",
    "            [0, num_classes) = foreground\n",
    "            num_classes      = background\n",
    "            -1               = ignore\n",
    "        num_samples (int): total number of samples\n",
    "        positive_fraction (float): fraction of positives\n",
    "        num_classes (int): number of foreground classes\n",
    "\n",
    "    Returns:\n",
    "        sampled_fg_idxs (Tensor)\n",
    "        sampled_bg_idxs (Tensor)\n",
    "    \"\"\"\n",
    "\n",
    "    # foreground: [0, num_classes)\n",
    "    fg_mask = (labels >= 0) & (labels < num_classes)\n",
    "    fg_idxs = torch.nonzero(fg_mask).squeeze(1)\n",
    "\n",
    "    # background: == num_classes\n",
    "    bg_mask = labels == num_classes\n",
    "    bg_idxs = torch.nonzero(bg_mask).squeeze(1)\n",
    "\n",
    "    num_fg = int(num_samples * positive_fraction)\n",
    "    num_fg = min(num_fg, fg_idxs.numel())\n",
    "\n",
    "    num_bg = num_samples - num_fg\n",
    "    num_bg = min(num_bg, bg_idxs.numel())\n",
    "\n",
    "    # Random sampling\n",
    "    perm_fg = torch.randperm(fg_idxs.numel(), device=labels.device)[:num_fg]\n",
    "    perm_bg = torch.randperm(bg_idxs.numel(), device=labels.device)[:num_bg]\n",
    "\n",
    "    sampled_fg_idxs = fg_idxs[perm_fg]\n",
    "    sampled_bg_idxs = bg_idxs[perm_bg]\n",
    "\n",
    "    return sampled_fg_idxs, sampled_bg_idxs\n",
    "\n",
    "class ROIHeads3D(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            num_classes: int,\n",
    "            batch_size_per_image: int,\n",
    "            positive_fraction: float,\n",
    "            proposal_matcher,\n",
    "            proposal_append_gt: bool,\n",
    "            roi_pooler,\n",
    "            box_head,\n",
    "            box_predictor,\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size_per_image = batch_size_per_image\n",
    "        self.positive_fraction = positive_fraction\n",
    "        self.proposal_matcher = proposal_matcher\n",
    "        self.proposal_append_gt = proposal_append_gt\n",
    "\n",
    "        self.roi_pooler = roi_pooler\n",
    "        self.box_head = box_head\n",
    "        self.box_predictor = box_predictor\n",
    "\n",
    "    def _sample_proposals(\n",
    "            self,\n",
    "            matched_idxs: torch.Tensor,\n",
    "            matched_labels: torch.Tensor,\n",
    "            gt_classes: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        has_gt = gt_classes.numel() > 0\n",
    "\n",
    "        if has_gt:\n",
    "            gt_classes = gt_classes[matched_idxs]\n",
    "            gt_classes[matched_labels == 0] = self.num_classes\n",
    "            gt_classes[matched_labels == -1] = -1\n",
    "        else:\n",
    "            gt_classes = torch.zeros_like(matched_idxs) + self.num_classes\n",
    "\n",
    "        sampled_fg_idxs, sampled_bg_idxs = subsample_labels(\n",
    "            gt_classes,\n",
    "            self.batch_size_per_image,\n",
    "            self.positive_fraction,\n",
    "            self.num_classes\n",
    "        )\n",
    "\n",
    "        sampled_idxs = torch.cat([sampled_fg_idxs, sampled_bg_idxs], dim=0)\n",
    "        return sampled_idxs, gt_classes[sampled_idxs]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def label_and_sample_proposals(\n",
    "        self,\n",
    "        proposals: List[Instances3D],\n",
    "        targets: List[Instances3D],\n",
    "    ) -> List[Instances3D]:\n",
    "        \n",
    "        if self.proposal_append_gt:\n",
    "            proposals = add_ground_truth_to_proposals_3d(targets, proposals)\n",
    "\n",
    "        proposals_with_gt = []\n",
    "\n",
    "        for proposal_per_image, targets_per_image in zip(proposals, targets):\n",
    "\n",
    "            has_gt = len(targets_per_image) > 0\n",
    "\n",
    "            if has_gt:\n",
    "                match_quality_matrix = pairwise_iou_3d(\n",
    "                    targets_per_image.gt_boxes,\n",
    "                    proposal_per_image.proposal_boxes\n",
    "                )\n",
    "\n",
    "                matched_idxs, matched_labels = self.proposal_matcher(match_quality_matrix)\n",
    "\n",
    "            else:\n",
    "                device = proposal_per_image.proposal_boxes.tensor.device\n",
    "                matched_idxs = torch.zeros(\n",
    "                    len(proposal_per_image), dtype=torch.int64, device=device\n",
    "\n",
    "                )\n",
    "\n",
    "                matched_labels = torch.zeros_like(matched_idxs)\n",
    "\n",
    "            sampled_idxs, gt_classes = self._sample_proposals(\n",
    "                matched_idxs,\n",
    "                matched_labels,\n",
    "                targets_per_image.gt_classes if has_gt else torch.empty(0),\n",
    "            )\n",
    "\n",
    "            proposals_per_image = proposal_per_image[sampled_idxs]\n",
    "            proposals_per_image.gt_classes = gt_classes\n",
    "\n",
    "            if has_gt:\n",
    "                sampled_targets = matched_idxs[sampled_idxs]\n",
    "                proposals_per_image.gt_boxes = targets_per_image.gt_boxes[sampled_targets]\n",
    "\n",
    "            proposals_with_gt.append(proposals_per_image)\n",
    "\n",
    "        return proposals_with_gt\n",
    "    \n",
    "    def forward(\n",
    "            self,\n",
    "            features: Dict[str, torch.Tensor],\n",
    "            proposals: List[Instances3D],\n",
    "            targets: Optional[List[Instances3D]] = None,\n",
    "            training: bool = True,\n",
    "    ):\n",
    "        \n",
    "        if training:\n",
    "            assert targets is not None\n",
    "            proposals = self.label_and_sample_proposals(proposals, targets)\n",
    "        \n",
    "        proposal_boxes = [p.proposal_boxes for p in proposals]\n",
    "        feature_tensors = [v for k, v in features.items()]\n",
    "\n",
    "        box_features = self.roi_pooler(feature_tensors, proposal_boxes)\n",
    "        # print(f\"Box features pool - {box_features.shape}\")\n",
    "        box_features = self.box_head(box_features)\n",
    "        # print(f\"Box features shape - {box_features.shape}\")\n",
    "\n",
    "        predictions = self.box_predictor(box_features)\n",
    "\n",
    "        if training:\n",
    "            return self.box_predictor.losses(predictions, proposals)\n",
    "        else:\n",
    "            return self.box_predictor.inference(predictions, proposals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12ab79a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "from qct_3d_nod_detect.layers import ShapeSpec\n",
    "from qct_3d_nod_detect.box_heads import FastRCNNConvFCHead3D\n",
    "import torch\n",
    "\n",
    "def random_valid_boxes_3d(num_boxes, image_size):\n",
    "    \"\"\"\n",
    "    Returns valid 3D boxes in (x1, y1, z1, x2, y2, z2) format\n",
    "    \"\"\"\n",
    "    D, H, W = image_size\n",
    "\n",
    "    x1 = torch.rand(num_boxes) * (W * 0.8)\n",
    "    y1 = torch.rand(num_boxes) * (H * 0.8)\n",
    "    z1 = torch.rand(num_boxes) * (D * 0.8)\n",
    "\n",
    "    w = torch.rand(num_boxes) * (W * 0.2) + 1.0\n",
    "    h = torch.rand(num_boxes) * (H * 0.2) + 1.0\n",
    "    d = torch.rand(num_boxes) * (D * 0.2) + 1.0\n",
    "\n",
    "    x2 = x1 + w\n",
    "    y2 = y1 + h\n",
    "    z2 = z1 + d\n",
    "\n",
    "    return torch.stack([x1, y1, z1, x2, y2, z2], dim=1)\n",
    "\n",
    "\n",
    "input_shape = ShapeSpec(\n",
    "    channels=256,\n",
    "    depth=7,\n",
    "    height=7,\n",
    "    width=7,\n",
    ")\n",
    "\n",
    "box_head = FastRCNNConvFCHead3D(\n",
    "    input_shape=input_shape,\n",
    "    conv_dims=[256, 256],\n",
    "    fc_dims=[512],\n",
    ")\n",
    "\n",
    "x = torch.randn(8, 256, 7, 7, 7)  # 8 ROIs\n",
    "y = box_head(x)\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "from qct_3d_nod_detect.structures import Instances3D, Boxes3D\n",
    "from qct_3d_nod_detect.matcher import Matcher\n",
    "import torch\n",
    "\n",
    "# Fake proposals\n",
    "proposals = []\n",
    "for _ in range(2):  # batch size = 2\n",
    "    inst = Instances3D(image_size=(128, 128, 128))\n",
    "    inst.proposal_boxes = Boxes3D(torch.rand(10, 6)) # 10 proposals\n",
    "    inst.objectness_logits = torch.rand(10)\n",
    "    proposals.append(inst)\n",
    "\n",
    "# Fake GT\n",
    "targets = []\n",
    "for _ in range(2):\n",
    "    inst = Instances3D(image_size=(128, 128, 128))\n",
    "    inst.gt_boxes = Boxes3D(random_valid_boxes_3d(3, (128,)*3)) # 3 GT boxes\n",
    "    inst.gt_classes = torch.randint(0, 1, (3,)) # 2 classes\n",
    "    targets.append(inst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24608595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qct_3d_nod_detect.poolers import ROIPooler3D\n",
    "from qct_3d_nod_detect.faster_rcnn import FasterRCNNOutputLayers3D\n",
    "from qct_3d_nod_detect.box_regression import Box3DTransform\n",
    "import math\n",
    "\n",
    "box3d2box3d_transform = Box3DTransform(\n",
    "    weights=(1.0, 1.0, 1.0, 1.0, 1.0, 1.0),\n",
    "    scale_clamp=math.log(1000.0),\n",
    ")\n",
    "\n",
    "proposal_matcher = Matcher(\n",
    "    thresholds=[0.1, 0.2],\n",
    "    labels = [0, -1, 1],\n",
    "    allow_low_quality_matches=True\n",
    ")\n",
    "\n",
    "roi_pooler = ROIPooler3D(\n",
    "    output_size=(7, 7, 7),\n",
    "    canonical_level=4,\n",
    "    canonical_box_size=224,\n",
    "    pooler_type=\"ROIALign3DV2\",\n",
    "    scales=[1, 2, 0.5, 0.25]\n",
    ")\n",
    "\n",
    "box_predictor = FasterRCNNOutputLayers3D(\n",
    "    input_dim=512,\n",
    "    num_classes=1,\n",
    "    box2box_transform=box3d2box3d_transform,\n",
    "    cls_agnostic_bbox_reg=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db91a29f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ROIHeads3D.__init__() got an unexpected keyword argument 'is_training'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m roi_heads = \u001b[43mROIHeads3D\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size_per_image\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpositive_fraction\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproposal_matcher\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproposal_matcher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproposal_append_gt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# IMPORTANT for early training\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroi_pooler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mroi_pooler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# your 3D ROI pooler\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbox_head\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbox_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;66;43;03m# FastRCNNConvFCHead3D\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbox_predictor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbox_predictor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# FasterRCNNOutputLayers3D\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_training\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: ROIHeads3D.__init__() got an unexpected keyword argument 'is_training'"
     ]
    }
   ],
   "source": [
    "roi_heads = ROIHeads3D(\n",
    "    num_classes=1,\n",
    "    batch_size_per_image=2,\n",
    "    positive_fraction=0.5,\n",
    "    proposal_matcher=proposal_matcher,\n",
    "    proposal_append_gt=True,       # IMPORTANT for early training\n",
    "    roi_pooler=roi_pooler,         # your 3D ROI pooler\n",
    "    box_head=box_head,             # FastRCNNConvFCHead3D\n",
    "    box_predictor=box_predictor,   # FasterRCNNOutputLayers3D\n",
    "    is_training=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "586d1906",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C = 2, 256\n",
    "\n",
    "features = {}\n",
    "features['p2'] = torch.rand(B, C, 32, 32, 32)\n",
    "features['p3'] = torch.rand(B, C, 16, 16, 16)\n",
    "features['p4'] = torch.rand(B, C, 8, 8, 8)\n",
    "features['p5'] = torch.rand(B, C, 4, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "197447a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box features pool - torch.Size([4, 256, 7, 7, 7])\n",
      "Box features shape - torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "losses = roi_heads(features, proposals, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adbe7f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss_cls': tensor(0.6722, grad_fn=<MulBackward0>),\n",
       " 'loss_box_reg': tensor(0.0004, grad_fn=<MulBackward0>)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da6d4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qct_nod_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
