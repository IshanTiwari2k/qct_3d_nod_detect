{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2295b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp backbones\n",
    "#| export\n",
    "from typing import Tuple, Dict, Literal, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "import math\n",
    "from qct_3d_nod_detect.SimpleFPN import BackboneFPN, SimpleFPN\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob: float = 0.0) -> None:\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "\n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(self, dim: int, init_values: float = 1e-5) -> None:\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
    "        return x * self.gamma\n",
    "\n",
    "\n",
    "class TubePatchEmbed3D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: Tuple[int, int, int],\n",
    "        patch_size: Tuple[int, int, int],\n",
    "        in_channels: int,\n",
    "        embed_dim: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (\n",
    "            img_size[0] // patch_size[0],\n",
    "            img_size[1] // patch_size[1],\n",
    "            img_size[2] // patch_size[2],\n",
    "        )\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1] * self.grid_size[2]\n",
    "        self.proj = nn.Conv3d(\n",
    "            in_channels,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
    "        x = self.proj(x)\n",
    "        b, d, z, y, xdim = x.shape\n",
    "        return x.permute(0, 2, 3, 4, 1).reshape(b, z * y * xdim, d)\n",
    "\n",
    "\n",
    "def get_3d_sincos_pos_embed(embed_dim: int, grid_size: Tuple[int, int, int]) -> torch.Tensor:\n",
    "    gz, gy, gx = grid_size\n",
    "    grid_z = torch.arange(gz, dtype=torch.float32)\n",
    "    grid_y = torch.arange(gy, dtype=torch.float32)\n",
    "    grid_x = torch.arange(gx, dtype=torch.float32)\n",
    "    grid = torch.stack(torch.meshgrid(grid_z, grid_y, grid_x, indexing=\"ij\"), dim=-1).reshape(-1, 3)\n",
    "\n",
    "    def _emb(dim_val: int, coord: torch.Tensor) -> torch.Tensor:\n",
    "        omega = torch.arange(dim_val // 2, dtype=torch.float32)\n",
    "        if len(omega) > 1:\n",
    "            omega = omega / (len(omega) - 1)\n",
    "        omega = 1.0 / (10000**omega)\n",
    "        out = torch.einsum(\"n,d->nd\", coord, omega)\n",
    "        return torch.cat([out.sin(), out.cos()], dim=1)\n",
    "\n",
    "    dim_z = embed_dim // 3\n",
    "    dim_y = embed_dim // 3\n",
    "    dim_x = embed_dim - dim_z - dim_y\n",
    "    pos = torch.cat(\n",
    "        [\n",
    "            _emb(dim_z, grid[:, 0]),\n",
    "            _emb(dim_y, grid[:, 1]),\n",
    "            _emb(dim_x, grid[:, 2]),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    if pos.shape[1] < embed_dim:\n",
    "        pad = embed_dim - pos.shape[1]\n",
    "        pos = torch.nn.functional.pad(pos, (0, pad))\n",
    "    return pos\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim: int, mlp_ratio: float, drop: float) -> None:\n",
    "        super().__init__()\n",
    "        hidden = int(dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(dim, hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden, dim)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        qkv_bias: bool,\n",
    "        attn_drop: float,\n",
    "        proj_drop: float,\n",
    "        use_sdpa: bool,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.use_sdpa = use_sdpa and hasattr(F, \"scaled_dot_product_attention\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
    "        b, n, c = x.shape\n",
    "        qkv = self.qkv(x).reshape(b, n, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        if self.use_sdpa:\n",
    "            x = F.scaled_dot_product_attention(q, k, v)\n",
    "        else:\n",
    "            attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            x = attn @ v\n",
    "        x = x.transpose(1, 2).reshape(b, n, c)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        drop: float = 0.0,\n",
    "        attn_drop: float = 0.0,\n",
    "        drop_path: float = 0.0,\n",
    "        use_layer_scale: bool = True,\n",
    "        layer_scale_init: float = 1e-5,\n",
    "        use_sdpa: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, num_heads, qkv_bias, attn_drop, drop, use_sdpa)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0 else nn.Identity()\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, mlp_ratio, drop)\n",
    "        self.use_layer_scale = use_layer_scale\n",
    "        if use_layer_scale:\n",
    "            self.ls1 = LayerScale(dim, layer_scale_init)\n",
    "            self.ls2 = LayerScale(dim, layer_scale_init)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
    "        if self.use_layer_scale:\n",
    "            x = x + self.drop_path(self.ls1(self.attn(self.norm1(x))))\n",
    "            x = x + self.drop_path(self.ls2(self.mlp(self.norm2(x))))\n",
    "        else:\n",
    "            x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT3D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: Tuple[int, int, int],\n",
    "        patch_size: Tuple[int, int, int],\n",
    "        in_channels: int,\n",
    "        embed_dim: int,\n",
    "        depth: int,\n",
    "        num_heads: int,\n",
    "        mlp_ratio: float,\n",
    "        qkv_bias: bool,\n",
    "        drop_rate: float,\n",
    "        attn_drop_rate: float,\n",
    "        drop_path_rate: float,\n",
    "        use_cls_token: bool,\n",
    "        use_sdpa: bool,\n",
    "        use_layer_scale: bool = True,\n",
    "        layer_scale_init: float = 1e-5,\n",
    "        num_register_tokens: int = 0,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.use_cls_token = use_cls_token\n",
    "        self.num_register_tokens = num_register_tokens\n",
    "\n",
    "        self.patch_embed = TubePatchEmbed3D(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.prefix_tokens = (1 if use_cls_token else 0) + num_register_tokens\n",
    "        self.grid_size = (\n",
    "            img_size[0] // patch_size[0],\n",
    "            img_size[1] // patch_size[1],\n",
    "            img_size[2] // patch_size[2],\n",
    "        )\n",
    "\n",
    "        if use_cls_token:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        if num_register_tokens > 0:\n",
    "            self.register_tokens = nn.Parameter(torch.zeros(1, num_register_tokens, embed_dim))\n",
    "\n",
    "        pos_embed = get_3d_sincos_pos_embed(embed_dim, self.grid_size)\n",
    "        seq = []\n",
    "        if use_cls_token:\n",
    "            seq.append(torch.zeros(1, embed_dim))\n",
    "        if num_register_tokens > 0:\n",
    "            seq.append(torch.zeros(num_register_tokens, embed_dim))\n",
    "        seq.append(pos_embed)\n",
    "        full_pos = torch.cat(seq, dim=0).unsqueeze(0)\n",
    "        self.pos_embed = nn.Parameter(full_pos, requires_grad=False)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = torch.linspace(0, drop_path_rate, depth).tolist()\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    drop=drop_rate,\n",
    "                    attn_drop=attn_drop_rate,\n",
    "                    drop_path=dpr[i],\n",
    "                    use_layer_scale=use_layer_scale,\n",
    "                    layer_scale_init=layer_scale_init,\n",
    "                    use_sdpa=use_sdpa,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.output_dim = embed_dim\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> dict[str, torch.Tensor]:  # type: ignore[override]\n",
    "        b = x.shape[0]\n",
    "        tokens = self.patch_embed(x)  # [B, N, D]\n",
    "        if self.use_cls_token:\n",
    "            cls_tokens = self.cls_token.expand(b, -1, -1)\n",
    "            tokens = torch.cat([cls_tokens, tokens], dim=1)\n",
    "        if self.num_register_tokens > 0:\n",
    "            regs = self.register_tokens.expand(b, -1, -1)\n",
    "            tokens = (\n",
    "                torch.cat([tokens[:, :1, :], regs, tokens[:, 1:, :]], dim=1)\n",
    "                if self.use_cls_token\n",
    "                else torch.cat([regs, tokens], dim=1)\n",
    "            )\n",
    "        tokens = tokens + self.pos_embed.to(tokens.dtype).to(tokens.device)\n",
    "        tokens = self.pos_drop(tokens)\n",
    "        for blk in self.blocks:\n",
    "            tokens = blk(tokens)\n",
    "        tokens = self.norm(tokens)\n",
    "        patch_tokens = tokens[:, self.prefix_tokens :, :]\n",
    "        global_token = tokens[:, 0] if self.use_cls_token else patch_tokens.mean(dim=1)\n",
    "        return {\n",
    "            \"feat_tokens\": tokens,\n",
    "            \"patch_tokens\": patch_tokens,\n",
    "            \"global\": global_token,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada280e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      7\u001b[39m         key = key[\u001b[38;5;28mlen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mencoder.\u001b[39m\u001b[33m\"\u001b[39m) :]\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m key\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_adapt_patch_embed_channels\u001b[39m(\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     weight: \u001b[43mtorch\u001b[49m.Tensor, target_shape: torch.Size, in_chans: \u001b[38;5;28mint\u001b[39m\n\u001b[32m     13\u001b[39m ) -> torch.Tensor | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m weight.ndim != \u001b[32m5\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m weight.shape[\u001b[32m2\u001b[39m:] != target_shape[\u001b[32m2\u001b[39m:]:\n\u001b[32m     15\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "def _clean_key(key: str) -> str:\n",
    "    for prefix in (\"model.\", \"module.\"):\n",
    "        if key.startswith(prefix):\n",
    "            key = key[len(prefix) :]\n",
    "    if key.startswith(\"encoder.\"):\n",
    "        key = key[len(\"encoder.\") :]\n",
    "    return key\n",
    "\n",
    "\n",
    "def _adapt_patch_embed_channels(\n",
    "    weight: torch.Tensor, target_shape: torch.Size, in_chans: int\n",
    ") -> torch.Tensor | None:\n",
    "    if weight.ndim != 5 or weight.shape[2:] != target_shape[2:]:\n",
    "        return None\n",
    "    out_channels, old_in, _, _, _ = weight.shape\n",
    "    if out_channels != target_shape[0]:\n",
    "        return None\n",
    "    if old_in == in_chans:\n",
    "        return weight\n",
    "    if in_chans == 1:\n",
    "        return weight.mean(dim=1, keepdim=True)\n",
    "    if in_chans < old_in:\n",
    "        trimmed = weight[:, :in_chans, :, :, :]\n",
    "        return trimmed * (old_in / in_chans)\n",
    "    repeat = math.ceil(in_chans / old_in)\n",
    "    expanded = weight.repeat(1, repeat, 1, 1, 1)[:, :in_chans, :, :, :]\n",
    "    expanded *= old_in / in_chans\n",
    "    return expanded\n",
    "\n",
    "def load_state_dict_into_vit(vit: ViT3D, ckpt_path: Path) -> tuple[list[str], list[str]]:\n",
    "\n",
    "    raw = torch.load(str(ckpt_path), map_location=\"cpu\", weights_only=False)\n",
    "    sd = raw.get(\"state_dict\", raw.get(\"model\", raw))\n",
    "    if not isinstance(sd, dict):\n",
    "        if hasattr(sd, \"state_dict\"):\n",
    "            sd = sd.state_dict()\n",
    "        else:\n",
    "            raise RuntimeError(\"Unsupported checkpoint format.\")\n",
    "    target = vit.state_dict()\n",
    "    cleaned: Dict[str, torch.Tensor] = {}\n",
    "    for k, v in sd.items():\n",
    "        key = _clean_key(k)\n",
    "        if key == \"pos_embed\" and v.shape != target.get(\"pos_embed\", v).shape:\n",
    "            continue\n",
    "        if (\n",
    "            key == \"patch_embed.proj.weight\"\n",
    "            and key in target\n",
    "            and v.shape[1] != target[key].shape[1]\n",
    "        ):\n",
    "            adapted = _adapt_patch_embed_channels(v, target[key].shape, target[key].shape[1])\n",
    "            if adapted is not None:\n",
    "                cleaned[key] = adapted\n",
    "                continue\n",
    "        if key in target and getattr(v, \"shape\", None) == target[key].shape:\n",
    "            cleaned[key] = v\n",
    "    missing, unexpected = vit.load_state_dict(cleaned, strict=False)\n",
    "    if missing:\n",
    "        logger.info(\"Missing MAE keys during load: %s\", missing)\n",
    "    if unexpected:\n",
    "        logger.info(\"Unexpected MAE keys during load: %s\", unexpected)\n",
    "    return list(missing), list(unexpected)\n",
    "\n",
    "def build_vit_backbone_with_fpn(\n",
    "    variant: Literal[\"S\", \"L\"],\n",
    "    ckpt_path: Union[Path, str],\n",
    "    scales: Tuple[float, ...] = (1, 2, 0.5, 0.25),\n",
    "    out_channels: int = 256,\n",
    ") -> nn.Module:\n",
    "\n",
    "    if variant == \"S\":\n",
    "        model_config = {\n",
    "            \"img_size\": (128, 128, 128),\n",
    "            \"patch_size\": (16, 16, 16),\n",
    "            \"in_channels\": 1,\n",
    "            \"embed_dim\": 384,\n",
    "            \"depth\": 12,\n",
    "            \"num_heads\": 6,\n",
    "            \"mlp_ratio\": 4.0,\n",
    "            \"qkv_bias\": True,\n",
    "            \"drop_rate\": 0.0,\n",
    "            \"attn_drop_rate\": 0.0,\n",
    "            \"drop_path_rate\": 0.1,\n",
    "            \"use_cls_token\": True,\n",
    "            \"use_sdpa\": True, # TODO Populated with sample, will change later\n",
    "        }\n",
    "\n",
    "    elif variant == \"L\":\n",
    "        model_config = {\n",
    "            \"img_size\": (128, 128, 128),\n",
    "            \"patch_size\": (8, 8, 8),\n",
    "            \"in_channels\": 1,\n",
    "            \"embed_dim\": 1024,\n",
    "            \"depth\": 24,\n",
    "            \"num_heads\": 16,\n",
    "            \"mlp_ratio\": 4.0,\n",
    "            \"qkv_bias\": True,\n",
    "            \"drop_rate\": 0.0,\n",
    "            \"attn_drop_rate\": 0.0,\n",
    "            \"drop_path_rate\": 0.0,\n",
    "            \"use_cls_token\": True,\n",
    "            \"use_sdpa\": True,\n",
    "        }\n",
    "\n",
    "\n",
    "    # Load backbone\n",
    "    backbone = ViT3D(**model_config)\n",
    "    if ckpt_path is not None:\n",
    "        if not isinstance(ckpt_path, Path):\n",
    "            ckpt_path = Path(ckpt_path)\n",
    "            print(f\"Loading from pretrained checkpoint - {ckpt_path}\")\n",
    "\n",
    "        missing, unexpected = load_state_dict_into_vit(backbone, ckpt_path)\n",
    "        print(f\"Missing keys: {missing}, Unexpected keys: {unexpected}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"loading random weights\")\n",
    "\n",
    "    # Build with FPN\n",
    "    fpn = SimpleFPN(dim=model_config[\"embed_dim\"], out_channels=out_channels, scales=scales)\n",
    "    backbone_fpn = BackboneFPN(\n",
    "        backbone=backbone,\n",
    "        fpn=fpn,\n",
    "        patch_grid_size=(\n",
    "            model_config[\"img_size\"][0] // model_config[\"patch_size\"][0],\n",
    "            model_config[\"img_size\"][1] // model_config[\"patch_size\"][1],\n",
    "            model_config[\"img_size\"][2] // model_config[\"patch_size\"][2],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return backbone_fpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73172966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading random weights\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "backbone_fpn = build_vit_backbone_with_fpn(\n",
    "    variant=\"S\",\n",
    "    ckpt_path=None,\n",
    "    scales=[1, 2, 0.5, 0.25],\n",
    "    out_channels=256\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61211cf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (513) must match the size of tensor b (385) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m x = torch.randn(\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m128\u001b[39m, \u001b[32m128\u001b[39m, \u001b[32m128\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m out = \u001b[43mbackbone_fpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Ishan_Nodseg/qct_3d_nod_detect/qct_3d_nod_detect/SimpleFPN.py:87\u001b[39m, in \u001b[36mBackboneFPN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     80\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03m        x: Tensor[B, 1, D, H, W]\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[33;03m        Dict[str, Tensor]: multi-scale features\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     feat = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     patch_tokens = feat[\u001b[33m'\u001b[39m\u001b[33mpatch_tokens\u001b[39m\u001b[33m'\u001b[39m]  \u001b[38;5;66;03m# B, N, C\u001b[39;00m\n\u001b[32m     89\u001b[39m     B, N, C = patch_tokens.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 275\u001b[39m, in \u001b[36mViT3D.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    269\u001b[39m     regs = \u001b[38;5;28mself\u001b[39m.register_tokens.expand(b, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m    270\u001b[39m     tokens = (\n\u001b[32m    271\u001b[39m         torch.cat([tokens[:, :\u001b[32m1\u001b[39m, :], regs, tokens[:, \u001b[32m1\u001b[39m:, :]], dim=\u001b[32m1\u001b[39m)\n\u001b[32m    272\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_cls_token\n\u001b[32m    273\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m torch.cat([regs, tokens], dim=\u001b[32m1\u001b[39m)\n\u001b[32m    274\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m tokens = \u001b[43mtokens\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_embed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m tokens = \u001b[38;5;28mself\u001b[39m.pos_drop(tokens)\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (513) must match the size of tensor b (385) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 128, 128, 128)\n",
    "out = backbone_fpn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f90974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6005d6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qct_nod_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
