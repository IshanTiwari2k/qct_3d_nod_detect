{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4012d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp poolers\n",
    "#| export\n",
    "import math\n",
    "from typing import List, Optional, Tuple\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from qct_3d_nod_detect.layers import nonzero_tuple, cat, shapes_to_tensor\n",
    "from qct_3d_nod_detect.structures import Boxes3D, Instances3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e425563a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def assign_boxes_to_levels_3d(\n",
    "        box_lists: List[Boxes3D],\n",
    "        min_level: int,\n",
    "        max_level: int,\n",
    "        canonical_box_size: float = 224.0,\n",
    "        canonical_level: int = 4,\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    volumes = torch.cat([b.volume() for b in box_lists])\n",
    "    box_sizes = volumes ** (1.0 / 3.0)\n",
    "\n",
    "    level_assignments = torch.floor(\n",
    "        canonical_level + torch.log2(box_sizes / canonical_box_size + 1e-6)\n",
    "    )\n",
    "\n",
    "    level_assignments = torch.clamp(level_assignments, min=min_level, max=max_level)\n",
    "    return (level_assignments - min_level).to(torch.int64)\n",
    "\n",
    "def _convert_boxes_to_pooler_format_3d(boxes: Tensor, sizes: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Low-level helper: prepends batch indices to the box coordinates.\n",
    "\n",
    "    Args:\n",
    "        boxes: (M_total, 6) tensor of [x1,y1,z1, x2,y2,z2]\n",
    "        sizes: (B,) tensor with number of boxes per image\n",
    "\n",
    "    Returns:\n",
    "        (M_total, 7) tensor of [batch_idx, x1,y1,z1, x2,y2,z2]\n",
    "    \"\"\"\n",
    "    sizes = sizes.to(device=boxes.device)\n",
    "    indices = torch.repeat_interleave(\n",
    "        torch.arange(len(sizes), dtype=boxes.dtype, device=boxes.device),\n",
    "        sizes\n",
    "    )\n",
    "    return cat([indices[:, None], boxes], dim=1)\n",
    "\n",
    "def convert_boxes_to_pooler_format_3d(box_lists: List[Boxes3D]) -> Tensor:\n",
    "    \"\"\"\n",
    "    Convert a list of per-image 3D box objects into the format expected by\n",
    "    3D RoI pooling / alignment operations.\n",
    "\n",
    "    Compatible with:\n",
    "    - Boxes3D.tensor: [x1, y1, z1, x2, y2, z2] (min/max corners)\n",
    "    - ROIAlign3D / RoIPool3D expecting [batch_idx, x1,y1,z1, x2,y2,z2]\n",
    "\n",
    "    Args:\n",
    "        box_lists (List[Boxes3D]):\n",
    "            List of N Boxes3D objects (N = batch size).\n",
    "            Each Boxes3D has .tensor of shape (num_boxes_i, 6)\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (M, 7), where M is total number of boxes across batch.\n",
    "        Columns: [batch_index, x1, y1, z1, x2, y2, z2]\n",
    "    \"\"\"\n",
    "    if len(box_lists) == 0:\n",
    "        # Return empty tensor with correct dtype/device\n",
    "        return torch.empty((0, 7), dtype=torch.float32, device=torch.device(\"cpu\"))\n",
    "\n",
    "    # Concatenate all box tensors from all images\n",
    "    all_boxes = torch.cat([b.tensor for b in box_lists], dim=0)  # (M_total, 6)\n",
    "\n",
    "    # Get number of boxes per image (tracing-friendly)\n",
    "    sizes = shapes_to_tensor([len(b) for b in box_lists], device=all_boxes.device)\n",
    "\n",
    "    return _convert_boxes_to_pooler_format_3d(all_boxes, sizes)\n",
    "\n",
    "class RoIPool3D(nn.Module):\n",
    "    \"\"\"\n",
    "    3D ROI Pooling module.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple[int]): (out_d, out_h, out_w), size of the output feature map\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size: Tuple[int, int, int]):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, features: torch.Tensor, rois: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (Tensor[N, C, D, H, W]): input volumetric feature maps\n",
    "            rois (Tensor[num_rois, 7]): ROI boxes with (batch_idx, z1, y1, x1, z2, y2, x2)\n",
    "                coordinates must be in **feature map scale**.\n",
    "\n",
    "        Returns:\n",
    "            Tensor[num_rois, C, out_d, out_h, out_w]: pooled features\n",
    "        \"\"\"\n",
    "        num_rois = rois.size(0)\n",
    "        N, C, _, _, _ = features.shape\n",
    "        out_d, out_h, out_w = self.output_size\n",
    "\n",
    "        pooled = torch.zeros(\n",
    "            (num_rois, C, out_d, out_h, out_w),\n",
    "            device=features.device,\n",
    "            dtype=features.dtype,\n",
    "        )\n",
    "\n",
    "        for i in range(num_rois):\n",
    "            batch_idx = int(rois[i, 0])\n",
    "            z1, y1, x1, z2, y2, x2 = rois[i, 1:]\n",
    "\n",
    "            # Crop ROI and apply adaptive max pooling\n",
    "            roi_feature = features[\n",
    "                batch_idx : batch_idx + 1,\n",
    "                :,\n",
    "                int(z1):int(z2),\n",
    "                int(y1):int(y2),\n",
    "                int(x1):int(x2),\n",
    "            ]\n",
    "\n",
    "            pooled[i] = F.adaptive_max_pool3d(roi_feature, self.output_size)\n",
    "\n",
    "        return pooled\n",
    "\n",
    "class ROIAlign3D(nn.Module):\n",
    "    def __init__(self, output_size, spatial_scale, sampling_ratio=-1, aligned=True):\n",
    "        \"\"\"\n",
    "        3D ROIAlign: trilinear align.\n",
    "\n",
    "        Args:\n",
    "            output_size (tuple): (out_d, out_h, out_w)\n",
    "            spatial_scale (float): scale bar for the input boxes\n",
    "            sampling_ratio (int): number of grid samples per output bin (<=0 means adaptive)\n",
    "            aligned (bool): if True, use the same alignment strategy as Detectron2\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.spatial_scale = spatial_scale\n",
    "        self.sampling_ratio = sampling_ratio\n",
    "        self.aligned = aligned\n",
    "\n",
    "    def forward(self, input: torch.Tensor, rois: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: (N, C, D, H, W)\n",
    "            rois: (num_rois, 7) = (batch_idx, x1,y1,z1,x2,y2,z2)\n",
    "        \"\"\"\n",
    "        assert rois.dim() == 2 and rois.size(1) == 7\n",
    "\n",
    "        num_rois = rois.size(0)\n",
    "        output = []\n",
    "\n",
    "        N, C, D, H, W = input.shape\n",
    "        out_d, out_h, out_w = self.output_size\n",
    "\n",
    "        for i in range(num_rois):\n",
    "            batch_idx = int(rois[i, 0].item())\n",
    "            x1, y1, z1, x2, y2, z2 = rois[i, 1:] * self.spatial_scale\n",
    "\n",
    "            if self.aligned:\n",
    "                # coordinate shift for correct alignment\n",
    "                x1 -= 0.5\n",
    "                y1 -= 0.5\n",
    "                z1 -= 0.5\n",
    "                x2 -= 0.5\n",
    "                y2 -= 0.5\n",
    "                z2 -= 0.5\n",
    "\n",
    "            # voxel grid spacing\n",
    "            d_step = (z2 - z1) / max(out_d, 1)\n",
    "            h_step = (y2 - y1) / max(out_h, 1)\n",
    "            w_step = (x2 - x1) / max(out_w, 1)\n",
    "\n",
    "            # Construct a sampling grid\n",
    "            d = torch.linspace(z1 + 0.5 * d_step, z2 - 0.5 * d_step, out_d, device=input.device)\n",
    "            h = torch.linspace(y1 + 0.5 * h_step, y2 - 0.5 * h_step, out_h, device=input.device)\n",
    "            w = torch.linspace(x1 + 0.5 * w_step, x2 - 0.5 * w_step, out_w, device=input.device)\n",
    "\n",
    "            grid_d, grid_h, grid_w = torch.meshgrid(d, h, w, indexing=\"ij\")\n",
    "\n",
    "            # Normalize to [-1,1] in each dimension\n",
    "            grid = torch.stack((grid_w / (W - 1) * 2 - 1,\n",
    "                                grid_h / (H - 1) * 2 - 1,\n",
    "                                grid_d / (D - 1) * 2 - 1), dim=-1)\n",
    "\n",
    "            grid = grid.unsqueeze(0)  # (1, out_d, out_h, out_w, 3)\n",
    "\n",
    "            # Sample with trilinear interpolation\n",
    "            roi_feat = F.grid_sample(\n",
    "                input[batch_idx : batch_idx + 1],  # shape (1, C, D, H, W)\n",
    "                grid,\n",
    "                mode=\"bilinear\",\n",
    "                padding_mode=\"zeros\",\n",
    "                align_corners=True,\n",
    "            )\n",
    "\n",
    "            output.append(roi_feat)\n",
    "\n",
    "        return torch.cat(output, dim=0)  # (num_rois, C, out_d, out_h, out_w)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"{self.__class__.__name__}(\"\n",
    "            f\"output_size={self.output_size}, \"\n",
    "            f\"spatial_scale={self.spatial_scale}, \"\n",
    "            f\"sampling_ratio={self.sampling_ratio}, \"\n",
    "            f\"aligned={self.aligned}\"\n",
    "            f\")\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83ddab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ROIPooler3D(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_size: Tuple[int, int, int],\n",
    "        scales: List[float],\n",
    "        sampling_ratio: int = 0,\n",
    "        pooler_type: str = \"ROIAlignV2\",\n",
    "        canonical_box_size: float = 224.0,\n",
    "        canonical_level: int = 4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if isinstance(output_size, int):\n",
    "            output_size = (output_size, output_size, output_size)\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.canonical_box_size = canonical_box_size\n",
    "        self.canonical_level = canonical_level\n",
    "\n",
    "        # Compute levels from scales (stride = 1/scale, assume power of 2)\n",
    "        strides = [1.0 / s for s in scales]\n",
    "        min_level = int(round(math.log2(strides[0])))\n",
    "        max_level = int(round(math.log2(strides[-1])))\n",
    "\n",
    "        assert max_level - min_level + 1 == len(scales)\n",
    "\n",
    "        self.min_level = min_level\n",
    "        self.max_level = max_level\n",
    "\n",
    "        # Create per level poolers\n",
    "        for scale in scales:\n",
    "            if pooler_type == \"ROIAlign3D\":\n",
    "                self.level_poolers = nn.ModuleList(\n",
    "                    ROIAlign3D(\n",
    "                        output_size,\n",
    "                        spatial_scale=scale,\n",
    "                        sampling_ratio=sampling_ratio,\n",
    "                        aligned=False,\n",
    "                    )\n",
    "                    for scale in scales\n",
    "                )\n",
    "\n",
    "            elif pooler_type == \"ROIALign3DV2\":\n",
    "                self.level_poolers = nn.ModuleList(\n",
    "                    ROIAlign3D(\n",
    "                        output_size,\n",
    "                        spatial_scale=scale,\n",
    "                        sampling_ratio=sampling_ratio,\n",
    "                        aligned=True,\n",
    "                    )\n",
    "                    for scale in scales\n",
    "                )\n",
    "\n",
    "            elif pooler_type == \"ROIPool3D\":\n",
    "                self.level_poolers = nn.ModuleList(\n",
    "                    RoIPool3D(output_size)\n",
    "                    for scale in scales\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown pooler type: {pooler_type}\")\n",
    "\n",
    "            self.pooler_type = pooler_type\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: List[torch.Tensor], \n",
    "            box_lists: List[Boxes3D]\n",
    "        ) -> torch.Tensor:\n",
    "\n",
    "            \"\"\"\n",
    "            Args:\n",
    "\n",
    "            x: List[Tensor]      [ (B,C,D,H,W), (B,C,D/2,H/2,W/2), ... ]\n",
    "            box_lists: List[Boxes3D]  per image, usually list of length B\n",
    "            \"\"\"\n",
    "\n",
    "            pooler_fmt_boxes = convert_boxes_to_pooler_format_3d(box_lists)\n",
    "\n",
    "            if len(box_lists) == 0:\n",
    "                return torch.zeros((0, x[0].shape[1], *self.output_size), device=x[0].device)\n",
    "\n",
    "            # Mutli_level box assignment\n",
    "            level_assignments = assign_boxes_to_levels_3d(\n",
    "                box_lists,\n",
    "                self.min_level,\n",
    "                self.max_level,\n",
    "                self.canonical_box_size,\n",
    "                self.canonical_level,\n",
    "            )\n",
    "\n",
    "            num_channels = x[0].shape[1]\n",
    "            output = torch.zeros(\n",
    "                (pooler_fmt_boxes.shape[0], num_channels, *self.output_size),\n",
    "                dtype=x[0].dtype, device=x[0].device\n",
    "            )\n",
    "\n",
    "            for lvl, pooler in enumerate(self.level_poolers):\n",
    "                inds = (level_assignments == lvl).nonzero(as_tuple=True)[0]\n",
    "                if len(inds) == 0:\n",
    "                    continue\n",
    "                boxes_lvl = pooler_fmt_boxes[inds]\n",
    "                feat_lvl = pooler(x[lvl], boxes_lvl)\n",
    "                output.index_put_((inds,), feat_lvl)\n",
    "\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "767598ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Anchors per level: [1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from qct_3d_nod_detect.anchor_generator_3d import DefaultAnchorGenerator3D\n",
    "from qct_3d_nod_detect.box_regression import Box3DTransform\n",
    "from qct_3d_nod_detect.matcher import Matcher\n",
    "from qct_3d_nod_detect.rpn import RPN3D, StandardRPNHead3d\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#  Reuse your existing setup\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "N = 2\n",
    "C = 256\n",
    "\n",
    "image_sizes = [(32, 128, 128), (32, 128, 128)]\n",
    "\n",
    "features = {\n",
    "    \"p3\": torch.randn(N, C, 16, 64, 64, device=device),   # stride ~8\n",
    "    \"p4\": torch.randn(N, C,  8, 32, 32, device=device),   # stride ~16\n",
    "    \"p5\": torch.randn(N, C,  4, 16, 16, device=device),   # stride ~32\n",
    "}\n",
    "\n",
    "class ImageList3D:\n",
    "    def __init__(self, image_sizes):\n",
    "        self.image_sizes = image_sizes\n",
    "\n",
    "images = ImageList3D(image_sizes)\n",
    "\n",
    "gt_instances = []\n",
    "\n",
    "for i in range(N):\n",
    "    inst = Instances3D(image_sizes[i])\n",
    "\n",
    "    inst.gt_boxes = Boxes3D(\n",
    "        torch.tensor(\n",
    "            [\n",
    "                [10, 20, 5, 40, 60, 20],\n",
    "                [50, 40, 10, 90, 100, 30]\n",
    "            ],\n",
    "            dtype=torch.float32,\n",
    "            device=device\n",
    "        )\n",
    "    )\n",
    "\n",
    "    gt_instances.append(inst)\n",
    "\n",
    "anchor_generator_3d = DefaultAnchorGenerator3D(\n",
    "    sizes=[[2], [4], [8]],\n",
    "    aspect_ratios_3d=[[(1.0, 1.0)], [(1.0, 1.0)], [(1.0, 1.0)]],\n",
    "    strides=[8, 16, 32],\n",
    "    offset=0.5,\n",
    ").to(device)\n",
    "\n",
    "print(\"Anchors per level:\", anchor_generator_3d.num_cell_anchors)\n",
    "\n",
    "box3d2box3d_transform = Box3DTransform(\n",
    "    weights=(1.0, 1.0, 1.0, 1.0, 1.0, 1.0),\n",
    "    scale_clamp=math.log(1000.0),\n",
    ")\n",
    "\n",
    "num_anchors = anchor_generator_3d.num_cell_anchors[0]  # same for all levels\n",
    "rpn_head_3d = StandardRPNHead3d(\n",
    "    in_channels=C,\n",
    "    num_anchors=num_anchors,\n",
    "    box_dim=6,\n",
    ").to(device)\n",
    "\n",
    "anchor_matcher = Matcher(\n",
    "    thresholds=[0.3, 0.7],\n",
    "    labels=[0, -1, 1],\n",
    "    allow_low_quality_matches=True,\n",
    ")\n",
    "\n",
    "rpn = RPN3D(\n",
    "    in_features=[\"p3\", \"p4\", \"p5\"],\n",
    "    head=rpn_head_3d,\n",
    "    anchor_generator=anchor_generator_3d,\n",
    "    anchor_matcher=anchor_matcher,\n",
    "    box3d_transform=box3d2box3d_transform,\n",
    "    batch_size_per_image=256,\n",
    "    positive_fraction=0.5,\n",
    "    pre_nms_topk=(200, 100),\n",
    "    post_nms_topk=(100, 50),\n",
    "    nms_thresh=0.5,\n",
    "    min_box_size=2.0,\n",
    "    box_reg_loss_type=\"smooth_l1\",\n",
    "    smooth_l1_beta=0.0,\n",
    ").to(device)\n",
    "\n",
    "rpn.eval()\n",
    "with torch.no_grad():\n",
    "    proposals, losses = rpn(images, features, gt_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "46bb0fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total proposals: 45\n"
     ]
    }
   ],
   "source": [
    "total_proposals = sum(len(p) for p in proposals)\n",
    "print(f\"Total proposals: {total_proposals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8293871a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooler = ROIPooler3D(\n",
    "    output_size     = (7, 7, 7),\n",
    "    scales          = [1/8.0, 1/16.0, 1/32.0],   # must match feature strides\n",
    "    sampling_ratio  = 0,                         # your impl ignores it anyway\n",
    "    pooler_type     = \"ROIALign3DV2\",            # or \"ROIAlign3D\" or \"ROIPool3D\"\n",
    "    canonical_box_size = 32.0,                   # smaller than 224 — your volumes are tiny\n",
    "    canonical_level = 1,                         # adjust depending on how you number levels\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b37654ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROIPooler3D created with pooler type: ROIALign3DV2\n",
      "Levels: 3 → 5\n"
     ]
    }
   ],
   "source": [
    "print(\"ROIPooler3D created with pooler type:\", pooler.pooler_type)\n",
    "print(\"Levels:\", pooler.min_level, \"→\", pooler.max_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7de130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_scale_features = [features[\"p3\"], features[\"p4\"], features[\"p5\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e41dd935",
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_boxes_list = [inst.proposal_boxes for inst in proposals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8ffb755",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled = pooler(\n",
    "    x          = multi_scale_features,\n",
    "    box_lists  = proposal_boxes_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49538047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([45, 256, 7, 7, 7])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e222fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qct_nod_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
