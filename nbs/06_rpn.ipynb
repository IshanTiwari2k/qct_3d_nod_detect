{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdcab055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp rpn\n",
    "#| export\n",
    "from torch import nn\n",
    "from typing import List, Union, Tuple, Dict, Optional\n",
    "import torch\n",
    "from qct_3d_nod_detect.anchor_generator_3d import build_anchor_generator_3d\n",
    "from qct_3d_nod_detect.structures import Boxes3D, Instances3D, pairwise_iou_3d\n",
    "from qct_3d_nod_detect.box_regression import Box3DTransform, _dense_box_regression_loss_3d\n",
    "from qct_3d_nod_detect.matcher import Matcher\n",
    "from qct_3d_nod_detect.layers import ShapeSpec, cat\n",
    "from qct_3d_nod_detect.sampling import subsample_labels\n",
    "import torch.nn.functional as F\n",
    "from qct_3d_nod_detect.memory import retry_if_cuda_oom\n",
    "from qct_3d_nod_detect.proposal_utils import find_top_rpn_proposals_3d\n",
    "\n",
    "class StandardRPNHead3d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels: int, num_anchors: int, box_dim: int = 6, conv_dims: List[int] = [-1]):\n",
    "\n",
    "        super().__init__()\n",
    "        cur_channels = in_channels\n",
    "\n",
    "        if len(conv_dims) == 1:\n",
    "            out_channels = cur_channels if conv_dims[0] == -1 else conv_dims[0]\n",
    "            self.conv = self._get_rpn_conv(cur_channels, out_channels) # Activation after conv\n",
    "            cur_channels = out_channels\n",
    "\n",
    "        else:\n",
    "            self.conv = nn.Sequential()\n",
    "            for k, conv_dim in enumerate(conv_dims):\n",
    "                out_channels = cur_channels if conv_dims==-1 else conv_dim\n",
    "                if out_channels <= 0:\n",
    "                    raise ValueError(f\"Conv output channels should be greater than 0. Got {out_channels}\")\n",
    "                \n",
    "                conv = self._get_rpn_conv(cur_channels, out_channels)\n",
    "                self.conv.add_module(f\"conv{k}\", conv)\n",
    "                cur_channels = out_channels\n",
    "\n",
    "        self.objectness_logits = nn.Conv3d(cur_channels, num_anchors, kernel_size=1, stride=1) # 1x1x1 conv for objectness logits\n",
    "        self.anchor_deltas = nn.Conv3d(cur_channels, num_anchors * box_dim, kernel_size=1, stride=1) # 1x1x1 conv for predicting box2box transform deltas\n",
    "\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Conv3d):\n",
    "                nn.init.normal_(layer.weight, std=0.01)\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def _get_rpn_conv(self, in_channels, out_channels):\n",
    "        conv = nn.Sequential(\n",
    "            nn.Conv3d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        return conv\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg, input_shape):\n",
    "\n",
    "        in_channels = [s.channels for s in input_shape]\n",
    "        assert len(set(in_channels)) == 1, \"Each level must have the same channel\"\n",
    "\n",
    "        in_channels = in_channels[0]\n",
    "        anchor_generator = build_anchor_generator_3d(cfg, input_shape) \n",
    "        num_anchors = anchor_generator.num_anchors\n",
    "        box_dim = anchor_generator.box_dim # Should be 6\n",
    "        assert len(set(num_anchors)) == 1, \"Each level must have the same number of anchors\"\n",
    "\n",
    "        return {\n",
    "            \"in_channels\": in_channels,\n",
    "            \"num_anchors\": num_anchors[0],\n",
    "            \"box_dim\": box_dim,\n",
    "            \"conv_dims\": cfg.MODEL.RPN.CONV_DIMS\n",
    "        }\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]):\n",
    "\n",
    "        pred_objectness_logits = []\n",
    "        pred_anchor_deltas = []\n",
    "\n",
    "        for x in features:\n",
    "            t = self.conv(x)\n",
    "            pred_objectness_logits.append(self.objectness_logits(t))\n",
    "            pred_anchor_deltas.append(self.anchor_deltas(t))\n",
    "\n",
    "        return pred_objectness_logits, pred_anchor_deltas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00cb440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_rpn = StandardRPNHead3d(in_channels=256, num_anchors=3, box_dim=6)\n",
    "\n",
    "features = [\n",
    "    torch.rand(1, 256, 32, 32, 32),\n",
    "    torch.rand(1, 256, 64, 64, 64),\n",
    "    torch.rand(1, 256, 128, 128, 128),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a270272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_objectness_logits, pred_anchor_deltas = standard_rpn(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da517545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 32, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_objectness_logits[0].shape\n",
    "pred_anchor_deltas[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed096b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_rpn_head_3d(input_shapes: List[ShapeSpec], cfg=None):\n",
    "    \"\"\"\n",
    "    Very simple 3D RPN head builder – no registry, just creates StandardRPNHead3D.\n",
    "    \"\"\"\n",
    "    # Assume all levels have same channels (very common in FPN-like setups)\n",
    "    in_channels = input_shapes[0].channels\n",
    "    assert all(s.channels == in_channels for s in input_shapes), \\\n",
    "        \"All feature levels must have the same number of channels\"\n",
    "\n",
    "    # You can get num_anchors from anchor generator later, or pass it\n",
    "    # For now we leave it to be set when instantiating RPN\n",
    "\n",
    "    return StandardRPNHead3d(\n",
    "        in_channels=in_channels,\n",
    "        num_anchors=9,           # ← temporary / example value – will be overridden\n",
    "        box_dim=6,\n",
    "        conv_dims=[-1]           # or get from cfg if you keep some config\n",
    "    )\n",
    "\n",
    "class RPN3D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: List[str],\n",
    "        head: nn.Module,\n",
    "        anchor_generator: nn.Module,\n",
    "        anchor_matcher: Matcher,\n",
    "        box3d_transform: Box3DTransform,  # 3D box transform\n",
    "        batch_size_per_image: int,\n",
    "        positive_fraction: float,\n",
    "        pre_nms_topk: Tuple[float, float],\n",
    "        post_nms_topk: Tuple[float, float],\n",
    "        nms_thresh: float = 0.7,\n",
    "        min_box_size: float = 0.0,\n",
    "        anchor_boundary_thresh: float = -1.0,\n",
    "        loss_weight: Union[float, Dict[str, float]] = 1.0,\n",
    "        box_reg_loss_type: str = \"smooth_l1\",\n",
    "        smooth_l1_beta: float = 0.0,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.rpn_head = head\n",
    "        self.anchor_generator = anchor_generator\n",
    "        self.anchor_matcher = anchor_matcher\n",
    "        self.box3d_transform = box3d_transform\n",
    "        self.batch_size_per_image = batch_size_per_image\n",
    "        self.positive_fraction = positive_fraction\n",
    "        self.pre_nms_topk = {True: pre_nms_topk[0], False: pre_nms_topk[1]}\n",
    "        self.post_nms_topk = {True: post_nms_topk[0], False: post_nms_topk[1]}\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.min_box_size = float(min_box_size)\n",
    "        self.anchor_boundary_thresh = anchor_boundary_thresh\n",
    "\n",
    "        if isinstance(loss_weight, float):\n",
    "            loss_weight = {\"loss_rpn_cls\": loss_weight, \"loss_rpn_loc\": loss_weight}\n",
    "        \n",
    "        self.loss_weight = loss_weight\n",
    "        self.box_reg_loss_type = box_reg_loss_type\n",
    "        self.smooth_l1_beta = smooth_l1_beta\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg, input_shape: Dict[str, ShapeSpec]):\n",
    "        in_features = cfg.MODEL.RPN.IN_FEATURES\n",
    "        ret = {\n",
    "            \"in_features\": in_features,\n",
    "            \"min_box_size\": cfg.MODEL.PROPOSAL_GENERATOR.MIN_SIZE,\n",
    "            \"nms_thresh\": cfg.MODEL.RPN.NMS_THRESH,\n",
    "            \"batch_size_per_image\": cfg.MODEL.RPN.BATCH_SIZE_PER_IMAGE,\n",
    "            \"positive_fraction\": cfg.MODEL.RPN.POSITIVE_FRACTION,\n",
    "            \"loss_weight\": {\n",
    "                \"loss_rpn_cls\": cfg.MODEL.RPN.LOSS_WEIGHT,\n",
    "                \"loss_rpn_loc\": cfg.MODEL.RPN.BBOX_REG_LOSS_WEIGHT * cfg.MODEL.RPN.LOSS_WEIGHT,\n",
    "            },\n",
    "            \"anchor_boundary_thresh\": cfg.MODEL.RPN.BOUNDARY_THRESH,\n",
    "            \"box3d_transform\": Box3DTransform(weights=cfg.MODEL.RPN.BBOX_REG_WEIGHTS),  # 3D weights (e.g., tuple of 6)\n",
    "            \"box_reg_loss_type\": cfg.MODEL.RPN.BBOX_REG_LOSS_TYPE,\n",
    "            \"smooth_l1_beta\": cfg.MODEL.RPN.SMOOTH_L1_BETA,\n",
    "        }\n",
    "        ret[\"pre_nms_topk\"] = (cfg.MODEL.RPN.PRE_NMS_TOPK_TRAIN, cfg.MODEL.RPN.PRE_NMS_TOPK_TEST)\n",
    "        ret[\"post_nms_topk\"] = (cfg.MODEL.RPN.POST_NMS_TOPK_TRAIN, cfg.MODEL.RPN.POST_NMS_TOPK_TEST)\n",
    "        ret[\"anchor_generator\"] = build_anchor_generator_3d(cfg, [input_shape[f] for f in in_features])\n",
    "        ret[\"anchor_matcher\"] = Matcher(\n",
    "            cfg.MODEL.RPN.IOU_THRESHOLDS, cfg.MODEL.RPN.IOU_LABELS, allow_low_quality_matches=True\n",
    "        )\n",
    "\n",
    "        ret[\"head\"] = build_rpn_head_3d([input_shape[f] for f in in_features])  # Should return StandardRPNHead3D\n",
    "        return ret\n",
    "\n",
    "    def _subsample_labels(self, label):\n",
    "\n",
    "        pos_idx, neg_idx = subsample_labels(\n",
    "            label, self.batch_size_per_image, self.positive_fraction, 0\n",
    "        )\n",
    "\n",
    "        label.fill_(-1)\n",
    "        label.scatter_(0, pos_idx, 1)\n",
    "        label.scatter_(0, neg_idx, 0)\n",
    "\n",
    "        return label\n",
    "\n",
    "    @torch.no_grad\n",
    "    def label_and_sample_anchors(\n",
    "        self, anchors: List[Boxes3D], gt_instances: List[Instances3D]\n",
    "    ) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "        \n",
    "        \"\"\"\n",
    "            Args:\n",
    "                anchors (list[Boxes]): anchors for each feature map.\n",
    "                gt_instances: the ground-truth instances for each image.\n",
    "\n",
    "            Returns:\n",
    "                list[Tensor]:\n",
    "                    List of #img tensors. i-th element is a vector of labels whose length is\n",
    "                    the total number of anchors across all feature maps R = sum(Hi * Wi * A).\n",
    "                    Label values are in {-1, 0, 1}, with meanings: -1 = ignore; 0 = negative\n",
    "                    class; 1 = positive class.\n",
    "                list[Tensor]:\n",
    "                    i-th element is a Rx4 tensor. The values are the matched gt boxes for each\n",
    "                    anchor. Values are undefined for those anchors not labeled as 1.\n",
    "        \"\"\"\n",
    "\n",
    "        anchors = Boxes3D.cat(anchors) \n",
    "        gt_boxes = [x.gt_boxes for x in gt_instances]\n",
    "        image_sizes = [x.image_size for x in gt_instances]\n",
    "        \n",
    "        del gt_instances\n",
    "\n",
    "        gt_labels = []\n",
    "        matched_gt_boxes = []\n",
    "\n",
    "        for image_size_i, gt_boxes_i in zip(image_sizes, gt_boxes):\n",
    "\n",
    "            match_quality_matrix = retry_if_cuda_oom(pairwise_iou_3d)(gt_boxes_i, anchors)\n",
    "            matched_idxs, gt_labels_i = retry_if_cuda_oom(self.anchor_matcher)(match_quality_matrix)\n",
    "            gt_labels_i = gt_labels_i.to(device=gt_boxes_i.device)\n",
    "\n",
    "            del match_quality_matrix\n",
    "\n",
    "            if self.anchor_boundary_thresh >= 0:\n",
    "                anchors_inside_image = anchors.inside_box_3d(image_size_i, self.anchor_boundary_thresh)\n",
    "                gt_labels_i[~anchors_inside_image] = -1\n",
    "\n",
    "            gt_labels_i = self._subsample_labels(gt_labels_i)\n",
    "\n",
    "            if len(gt_labels_i) == 0:\n",
    "                matched_gt_boxes_i = torch.zeros_like(anchors.tensors)\n",
    "            else:\n",
    "                matched_gt_boxes_i = gt_boxes_i[matched_idxs].tensor\n",
    "                \n",
    "            gt_labels.append(gt_labels_i)\n",
    "            matched_gt_boxes.append(matched_gt_boxes_i)\n",
    "\n",
    "        return gt_labels, matched_gt_boxes\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def losses(\n",
    "        self,\n",
    "        anchors: List[Boxes3D],\n",
    "        pred_objectness_logits: List[torch.Tensor],\n",
    "        pred_anchor_deltas: List[torch.Tensor],\n",
    "        gt_labels: List[torch.Tensor],\n",
    "        gt_boxes: List[torch.Tensor]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        \"\"\"\n",
    "        Return the losses from a set of RPN predictions and their associated ground-truth.\n",
    "\n",
    "        Args:\n",
    "            anchors (list[Boxes or RotatedBoxes]): anchors for each feature map, each\n",
    "                has shape (Hi*Wi*A, B), where B is box dimension (4 or 5).\n",
    "            pred_objectness_logits (list[Tensor]): A list of L elements.\n",
    "                Element i is a tensor of shape (N, Hi*Wi*A) representing\n",
    "                the predicted objectness logits for all anchors.\n",
    "            gt_labels (list[Tensor]): Output of :meth:`label_and_sample_anchors`.\n",
    "            pred_anchor_deltas (list[Tensor]): A list of L elements. Element i is a tensor of shape\n",
    "                (N, Hi*Wi*A, 6) representing the predicted \"deltas\" used to transform anchors\n",
    "                to proposals.\n",
    "            gt_boxes (list[Tensor]): Output of :meth:`label_and_sample_anchors`.\n",
    "\n",
    "        Returns:\n",
    "            dict[loss name -> loss value]: A dict mapping from loss name to loss value.\n",
    "                Loss names are: `loss_rpn_cls` for objectness classification and\n",
    "                `loss_rpn_loc` for proposal localization.\n",
    "        \"\"\"\n",
    "\n",
    "        num_images = len(gt_labels)\n",
    "        gt_labels = torch.stack(gt_labels)\n",
    "\n",
    "        # Log the number of positive/negative anchors per-image that's used in training\n",
    "        pos_mask = gt_labels == 1\n",
    "        num_pos_anchors = pos_mask.sum().item()\n",
    "        num_neg_anchors = (gt_labels == 0).sum().item()\n",
    "\n",
    "        localization_loss = _dense_box_regression_loss_3d(\n",
    "            anchors,\n",
    "            self.box3d_transform,\n",
    "            pred_anchor_deltas,\n",
    "            gt_boxes,\n",
    "            pos_mask,\n",
    "            box_reg_loss_type=self.box_reg_loss_type,\n",
    "            smooth_l1_beta=self.smooth_l1_beta\n",
    "        )\n",
    "\n",
    "        valid_mask = gt_labels >= 0\n",
    "\n",
    "        objectness_loss = F.binary_cross_entropy_with_logits(\n",
    "            cat(pred_objectness_logits, dim=1)[valid_mask],\n",
    "            gt_labels[valid_mask].to(torch.float32),\n",
    "            reduction=\"sum\"\n",
    "        )\n",
    "\n",
    "        normalizer = self.batch_size_per_image * num_images\n",
    "        losses = {\n",
    "            \"loss_rpn_cls\": objectness_loss / normalizer,\n",
    "            \"loss_rpn_loc\": localization_loss / normalizer,\n",
    "        }\n",
    "\n",
    "        losses = {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images,\n",
    "        features: Dict[str, torch.Tensor],\n",
    "        gt_instances: Optional[List[Instances3D]] = None,\n",
    "        training: bool = True\n",
    "    ):\n",
    "\n",
    "        features = [features[f] for f in self.in_features]\n",
    "        anchors = self.anchor_generator(features)\n",
    "        anchors = [Boxes3D(anchor) for anchor in anchors]\n",
    "\n",
    "        # return anchors\n",
    "\n",
    "        pred_objectness_logits, pred_anchor_deltas = self.rpn_head(features)\n",
    "\n",
    "        # Objectness logits, \n",
    "        # (N, A, D, H, W) -> (N, D, H, W, A) -> (N, D*H*W*A)\n",
    "        pred_objectness_logits = [\n",
    "            score.permute(0, 2, 3, 4, 1).flatten(1)\n",
    "            for score in pred_objectness_logits\n",
    "        ]\n",
    "\n",
    "        # Box Deltas\n",
    "        # (N, A*6, D, H, W)\n",
    "        # -> (N, A, 6, D, H, W)\n",
    "        # -> (N, D, H, W, A, 6)\n",
    "        # -> (N, D*H*W*A, 6)\n",
    "        pred_anchor_deltas = [\n",
    "            x.view(x.shape[0], -1, self.anchor_generator.box_dim, x.shape[-3], x.shape[-2], x.shape[-1])\n",
    "            .permute(0, 3, 4, 5, 1, 2)\n",
    "            .flatten(1, -2)\n",
    "            for x in pred_anchor_deltas\n",
    "        ]\n",
    "\n",
    "        if training:\n",
    "\n",
    "            assert gt_instances is not None, \"RPN3D requires gt_instances in training!\"\n",
    "\n",
    "            gt_labels, gt_boxes = self.label_and_sample_anchors(\n",
    "                anchors, gt_instances\n",
    "            )\n",
    "\n",
    "            losses = self.losses(\n",
    "                anchors,\n",
    "                pred_objectness_logits,\n",
    "                pred_anchor_deltas,\n",
    "                gt_labels,\n",
    "                gt_boxes\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            losses = {}\n",
    "\n",
    "        proposals = self.predict_proposals(\n",
    "            anchors,\n",
    "            pred_objectness_logits,\n",
    "            pred_anchor_deltas,\n",
    "            images.image_sizes,\n",
    "            training\n",
    "        )\n",
    "\n",
    "        return proposals, losses\n",
    "\n",
    "    def predict_proposals(\n",
    "        self,\n",
    "        anchors: List[Boxes3D],\n",
    "        pred_objectness_logits: List[torch.Tensor],\n",
    "        pred_anchor_deltas: List[torch.Tensor],\n",
    "        image_sizes: List[Tuple[int, int]],\n",
    "        training: bool = True,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Decode all the predicted box regression deltas to proposals. Find the top proposals\n",
    "        by applying NMS and removing boxes that are too small.\n",
    "\n",
    "        Returns:\n",
    "            proposals (list[Instances]): list of N Instances. The i-th Instances\n",
    "                stores post_nms_topk object proposals for image i, sorted by their\n",
    "                objectness score in descending order.\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_proposals = self._decode_proposals_3d(anchors, pred_anchor_deltas)\n",
    "\n",
    "            return find_top_rpn_proposals_3d(\n",
    "                pred_proposals, \n",
    "                pred_objectness_logits,\n",
    "                image_sizes,\n",
    "                self.nms_thresh,\n",
    "                self.pre_nms_topk[training],\n",
    "                self.post_nms_topk[training],\n",
    "                self.min_box_size,\n",
    "                training\n",
    "            )\n",
    "\n",
    "    def _decode_proposals_3d(self, anchors: List[Boxes3D], pred_anchor_deltas: List[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Transform anchors into proposals by applying the predicted anchor deltas.\n",
    "\n",
    "        Returns:\n",
    "            proposals (list[Tensor]): A list of L tensors. Tensor i has shape\n",
    "                (N, Hi*Wi*A, B)\n",
    "        \"\"\"\n",
    "\n",
    "        N = pred_anchor_deltas[0].shape[0]\n",
    "        proposals = []\n",
    "\n",
    "        for anchors_i, pred_anchor_deltas_i in zip(anchors, pred_anchor_deltas):\n",
    "            B = anchors_i.tensor.size(1)\n",
    "            pred_anchor_deltas_i = pred_anchor_deltas_i.reshape(-1, B)\n",
    "\n",
    "            anchors_i = anchors_i.tensor.unsqueeze(0).expand(N, -1, -1).reshape(-1, B)\n",
    "            proposals_i = self.box3d_transform.apply_deltas(\n",
    "                pred_anchor_deltas_i, anchors_i\n",
    "            )\n",
    "\n",
    "            proposals.append(proposals_i.view(N, -1, B))\n",
    "        \n",
    "        return proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640d8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qct_nod_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
