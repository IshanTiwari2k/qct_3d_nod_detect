{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5cb85e",
   "metadata": {},
   "source": [
    "## Create toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a660574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3141994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sphere_volume(\n",
    "    shape=(64, 64, 64),\n",
    "    radius_range=(4, 8),\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        volume: (1, D, H, W)\n",
    "        box: (6,) -> (cx, cy, cz, dx, dy, dz)\n",
    "    \"\"\"\n",
    "    D, H, W = shape\n",
    "    volume = torch.zeros(1, D, H, W)\n",
    "\n",
    "    r = random.randint(*radius_range)\n",
    "    cx = random.randint(r, W - r - 1)\n",
    "    cy = random.randint(r, H - r - 1)\n",
    "    cz = random.randint(r, D - r - 1)\n",
    "\n",
    "    z, y, x = torch.meshgrid(\n",
    "        torch.arange(D),\n",
    "        torch.arange(H),\n",
    "        torch.arange(W),\n",
    "        indexing=\"ij\",\n",
    "    )\n",
    "\n",
    "    mask = (x - cx) ** 2 + (y - cy) ** 2 + (z - cz) ** 2 <= r ** 2\n",
    "    volume[0][mask] = 1.0\n",
    "\n",
    "    # Bounding box in (cx, cy, cz, dx, dy, dz)\n",
    "    x1, x2 = cx - r, cx + r\n",
    "    y1, y2 = cy - r, cy + r\n",
    "    z1, z2 = cz - r, cz + r\n",
    "\n",
    "    box = torch.tensor(\n",
    "        [\n",
    "            (x1 + x2) / 2,\n",
    "            (y1 + y2) / 2,\n",
    "            (z1 + z2) / 2,\n",
    "            x2 - x1,\n",
    "            y2 - y1,\n",
    "            z2 - z1,\n",
    "        ],\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    return volume, box\n",
    "\n",
    "def create_toy_detection_dataset(\n",
    "    root_dir,\n",
    "    num_train=500,\n",
    "    num_val=100,\n",
    "    shape=(64, 64, 64),\n",
    "):\n",
    "    root = Path(root_dir)\n",
    "\n",
    "    for split, n_samples in [(\"train\", num_train), (\"val\", num_val)]:\n",
    "        vol_dir = root / split / \"volumes\"\n",
    "        tgt_dir = root / split / \"targets\"\n",
    "        vol_dir.mkdir(parents=True, exist_ok=True)\n",
    "        tgt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            volume, box = generate_sphere_volume(shape)\n",
    "\n",
    "            torch.save(\n",
    "                volume,\n",
    "                vol_dir / f\"sample_{i:04d}.pt\",\n",
    "            )\n",
    "\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"boxes\": box.unsqueeze(0),      # (1, 6)\n",
    "                    \"labels\": torch.tensor([0]),    # single class\n",
    "                },\n",
    "                tgt_dir / f\"sample_{i:04d}.pt\",\n",
    "            )\n",
    "\n",
    "    print(f\"Dataset written to: {root}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e2b2149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset written to: /home/users/ishan.tiwari/Ishan_Nodseg/qct_3d_nod_detect/toy_dataset\n"
     ]
    }
   ],
   "source": [
    "create_toy_detection_dataset(\n",
    "    root_dir=\"/home/users/ishan.tiwari/Ishan_Nodseg/qct_3d_nod_detect/toy_dataset\",\n",
    "    num_train=1000,\n",
    "    num_val=200,\n",
    "    shape=(128,)*3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fff7faaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def visualize_3d_bbox(\n",
    "    root_dir,\n",
    "    split=\"train\",\n",
    "    sample_name=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Interactive visualization for a 3D volume with 3D bounding box.\n",
    "    \n",
    "    Expected structure:\n",
    "    root_dir/\n",
    "        train/\n",
    "            volumes/sample_xxxx.pt\n",
    "            targets/sample_xxxx.pt\n",
    "        val/\n",
    "            volumes/sample_xxxx.pt\n",
    "            targets/sample_xxxx.pt\n",
    "\n",
    "    target format:\n",
    "    {\n",
    "        'boxes': tensor([[x, y, z, dx, dy, dz]]),\n",
    "        'labels': tensor([class_id])\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    root_dir = Path(root_dir)\n",
    "    vol_dir = root_dir / split / \"volumes\"\n",
    "    tgt_dir = root_dir / split / \"targets\"\n",
    "\n",
    "    samples = sorted([p.stem for p in vol_dir.glob(\"*.pt\")])\n",
    "    if len(samples) == 0:\n",
    "        raise RuntimeError(\"No samples found\")\n",
    "\n",
    "    if sample_name is None:\n",
    "        sample_name = samples[0]\n",
    "\n",
    "    vol = torch.load(vol_dir / f\"{sample_name}.pt\")\n",
    "    tgt = torch.load(tgt_dir / f\"{sample_name}.pt\")\n",
    "\n",
    "    vol = vol.squeeze().cpu()\n",
    "    box = tgt[\"boxes\"][0].cpu()  # [x, y, z, dx, dy, dz]\n",
    "\n",
    "    x, y, z, dx, dy, dz = box\n",
    "    x0, x1 = int(x - dx / 2), int(x + dx / 2)\n",
    "    y0, y1 = int(y - dy / 2), int(y + dy / 2)\n",
    "    z0, z1 = int(z - dz / 2), int(z + dz / 2)\n",
    "\n",
    "    x0, y0, z0 = max(x0, 0), max(y0, 0), max(z0, 0)\n",
    "    x1, y1, z1 = min(x1, vol.shape[2]-1), min(y1, vol.shape[1]-1), min(z1, vol.shape[0]-1)\n",
    "\n",
    "    def plot_slice(axis, idx):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "\n",
    "        if axis == \"z\":\n",
    "            img = vol[idx]\n",
    "            if z0 <= idx <= z1:\n",
    "                plt.gca().add_patch(\n",
    "                    plt.Rectangle(\n",
    "                        (x0, y0),\n",
    "                        x1 - x0,\n",
    "                        y1 - y0,\n",
    "                        fill=False,\n",
    "                        edgecolor=\"red\",\n",
    "                        linewidth=2,\n",
    "                    )\n",
    "                )\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "\n",
    "        elif axis == \"y\":\n",
    "            img = vol[:, idx, :]\n",
    "            if y0 <= idx <= y1:\n",
    "                plt.gca().add_patch(\n",
    "                    plt.Rectangle(\n",
    "                        (x0, z0),\n",
    "                        x1 - x0,\n",
    "                        z1 - z0,\n",
    "                        fill=False,\n",
    "                        edgecolor=\"red\",\n",
    "                        linewidth=2,\n",
    "                    )\n",
    "                )\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "\n",
    "        elif axis == \"x\":\n",
    "            img = vol[:, :, idx]\n",
    "            if x0 <= idx <= x1:\n",
    "                plt.gca().add_patch(\n",
    "                    plt.Rectangle(\n",
    "                        (y0, z0),\n",
    "                        y1 - y0,\n",
    "                        z1 - z0,\n",
    "                        fill=False,\n",
    "                        edgecolor=\"red\",\n",
    "                        linewidth=2,\n",
    "                    )\n",
    "                )\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "\n",
    "        plt.title(f\"{sample_name} | axis={axis} | slice={idx}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    axis_dd = widgets.Dropdown(\n",
    "        options=[\"z\", \"y\", \"x\"],\n",
    "        value=\"z\",\n",
    "        description=\"Axis:\",\n",
    "    )\n",
    "\n",
    "    slice_slider = widgets.IntSlider(\n",
    "        min=0,\n",
    "        max=vol.shape[0] - 1,\n",
    "        step=1,\n",
    "        value=int(z),\n",
    "        description=\"Slice:\",\n",
    "        continuous_update=False,\n",
    "    )\n",
    "\n",
    "    def update_slider(*args):\n",
    "        axis = axis_dd.value\n",
    "        if axis == \"z\":\n",
    "            slice_slider.max = vol.shape[0] - 1\n",
    "            slice_slider.value = int(z)\n",
    "        elif axis == \"y\":\n",
    "            slice_slider.max = vol.shape[1] - 1\n",
    "            slice_slider.value = int(y)\n",
    "        elif axis == \"x\":\n",
    "            slice_slider.max = vol.shape[2] - 1\n",
    "            slice_slider.value = int(x)\n",
    "\n",
    "    axis_dd.observe(update_slider, names=\"value\")\n",
    "\n",
    "    ui = widgets.VBox([axis_dd, slice_slider])\n",
    "    out = widgets.interactive_output(\n",
    "        plot_slice,\n",
    "        {\"axis\": axis_dd, \"idx\": slice_slider},\n",
    "    )\n",
    "\n",
    "    display(ui, out)\n",
    "\n",
    "\n",
    "# Example usage in notebook:\n",
    "# visualize_3d_bbox(\n",
    "#     root_dir=\"/path/to/dataset\",\n",
    "#     split=\"train\",\n",
    "#     sample_name=\"sample_0001\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc743681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0f6bcdd17e48c99230850bc86ab8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Axis:', options=('z', 'y', 'x'), value='z'), IntSlider(value=16, continuoâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8e8d9cc46848bd9ef54e5e7a1fb6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_3d_bbox(root_dir=\"/home/users/ishan.tiwari/Ishan_Nodseg/qct_3d_nod_detect/toy_dataset\", split=\"train\", sample_name=\"sample_0011\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab3864",
   "metadata": {},
   "source": [
    "## Load the toy sphere dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46295325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from qct_3d_nod_detect.structures import Instances3D, Boxes3D\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "\n",
    "class GeneralizedRCNN3D(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_fpn,\n",
    "        rpn,\n",
    "        roi_pooler,\n",
    "        roi_head,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone_fpn = backbone_fpn\n",
    "        self.rpn = rpn\n",
    "        self.roi_pooler = roi_pooler\n",
    "        self.roi_head = roi_head\n",
    "\n",
    "    def forward(self, images, gt_instances=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Tensor[B, 1, D, H, W]\n",
    "            gt_instances: Optional[List[Instances3D]]\n",
    "        \"\"\"\n",
    "\n",
    "        # ----------------------------------\n",
    "        # Wrap images as ImageList3d\n",
    "        # ----------------------------------\n",
    "        image_list = build_image_list_3d(images)\n",
    "\n",
    "        # ----------------------------------\n",
    "        # Backbone + FPN\n",
    "        # ----------------------------------\n",
    "        features = self.backbone_fpn(images)\n",
    "        # Dict[str, Tensor]\n",
    "\n",
    "        # ----------------------------------\n",
    "        # RPN (uses ImageList3d + Instances3D)\n",
    "        # ----------------------------------\n",
    "        proposals, rpn_losses = self.rpn(\n",
    "            image_list,\n",
    "            features,\n",
    "            gt_instances if self.training else None,\n",
    "        )\n",
    "\n",
    "        proposal_boxes = [x.proposal_boxes for x in proposals]\n",
    "\n",
    "        # ----------------------------------\n",
    "        # ROI Pooling\n",
    "        # ----------------------------------\n",
    "        x = list(features.values())\n",
    "        roi_features = self.roi_pooler(\n",
    "            x,\n",
    "            proposal_boxes,\n",
    "        )\n",
    "\n",
    "        # ----------------------------------\n",
    "        # ROI Head\n",
    "        # ----------------------------------\n",
    "        predictions = self.roi_head(roi_features)\n",
    "\n",
    "        if self.training:\n",
    "            \n",
    "            roi_losses = self.roi_head.losses(predictions, proposals)\n",
    "\n",
    "            losses = {}\n",
    "            losses.update(rpn_losses)\n",
    "            losses.update(roi_losses)\n",
    "            return losses\n",
    "\n",
    "        else:\n",
    "            detections, _ = self.roi_head.inference(predictions, proposals)\n",
    "            return detections\n",
    "\n",
    "from qct_3d_nod_detect.structures import ImageList3D\n",
    "\n",
    "def build_image_list_3d(images: torch.Tensor) -> ImageList3D:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        images: Tensor[B, C, D, H, W]\n",
    "    \"\"\"\n",
    "    image_sizes = [tuple(images.shape[-3:]) for _ in range(images.shape[0])]\n",
    "    return ImageList3D(image_sizes)\n",
    "\n",
    "def build_instances_3d(batch):\n",
    "    instances = []\n",
    "\n",
    "    for boxes, classes in zip(batch[\"gt_boxes\"], batch[\"gt_classes\"]):\n",
    "        inst = Instances3D(image_size=batch[\"image\"].shape[-3:])\n",
    "        inst.gt_boxes = Boxes3D(boxes)\n",
    "        inst.gt_classes = classes\n",
    "        instances.append(inst)\n",
    "\n",
    "    return instances\n",
    "\n",
    "# Lightning module\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "class FasterRCNN3DLightning(pl.LightningModule):\n",
    "    def __init__(self, model, lr=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images = batch[\"image\"]                # Tensor[B, 1, D, H, W]\n",
    "        gt_instances = build_instances_3d(batch)\n",
    "\n",
    "        losses = self.model(images, gt_instances)\n",
    "\n",
    "        total_loss = sum(losses.values())\n",
    "\n",
    "        self.log_dict(\n",
    "            {k: v.detach() for k, v in losses.items()},\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\"loss_total\", total_loss, prog_bar=True)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0ea0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "class ToySphereDetectionDataset(Dataset):\n",
    "    def __init__(self, root_dir, split=\"train\"):\n",
    "        self.root = Path(root_dir) / split\n",
    "        self.vol_dir = self.root / \"volumes\"\n",
    "        self.tgt_dir = self.root / \"targets\"\n",
    "\n",
    "        self.ids = sorted(p.stem for p in self.vol_dir.glob(\"*.pt\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sid = self.ids[idx]\n",
    "\n",
    "        volume = torch.load(self.vol_dir / f\"{sid}.pt\")   # (1, D, H, W)\n",
    "        target = torch.load(self.tgt_dir / f\"{sid}.pt\")\n",
    "\n",
    "        return {\n",
    "            \"image\": volume,                  # Tensor[1, D, H, W]\n",
    "            \"gt_boxes\": target[\"boxes\"],      # Tensor[N, 6]\n",
    "            \"gt_classes\": target[\"labels\"],   # Tensor[N]\n",
    "        }\n",
    "    \n",
    "class ToySphereDetectionDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        batch_size: int = 2,\n",
    "        num_workers: int = 4,\n",
    "        pin_memory: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Called once per process\n",
    "        self.train_dataset = ToySphereDetectionDataset(\n",
    "            root_dir=self.root_dir,\n",
    "            split=\"train\",\n",
    "        )\n",
    "\n",
    "        self.val_dataset = ToySphereDetectionDataset(\n",
    "            root_dir=self.root_dir,\n",
    "            split=\"val\",\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=detection_collate,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=detection_collate,\n",
    "        )\n",
    "\n",
    "def detection_collate(batch):\n",
    "    return {\n",
    "        \"image\": torch.stack([b[\"image\"] for b in batch], dim=0),\n",
    "        \"gt_boxes\": [b[\"gt_boxes\"] for b in batch],\n",
    "        \"gt_classes\": [b[\"gt_classes\"] for b in batch],\n",
    "    }\n",
    "\n",
    "train_dataset = ToySphereDetectionDataset(\n",
    "    root_dir=\"/home/users/ishan.tiwari/Ishan_Nodseg/qct_3d_nod_detect/toy_dataset\",\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "val_dataset = ToySphereDetectionDataset(\n",
    "    root_dir=\"/home/users/ishan.tiwari/Ishan_Nodseg/qct_3d_nod_detect/toy_dataset\",\n",
    "    split=\"val\",\n",
    ")\n",
    "\n",
    "datamodule = ToySphereDetectionDataModule(root_dir='/home/users/ishan.tiwari/Ishan_Nodseg/qct_3d_nod_detect/toy_dataset',\n",
    "                                          batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de81fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,          # start small\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=detection_collate,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=detection_collate,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c89757d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 128, 128, 128])\n",
      "2\n",
      "torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "print(batch[\"image\"].shape)       # (B, 1, D, H, W)\n",
    "print(len(batch[\"gt_boxes\"]))     # B\n",
    "print(batch[\"gt_boxes\"][0].shape) # (N, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98831279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qct_3d_nod_detect.rpn import RPN3D, StandardRPNHead3d\n",
    "from qct_3d_nod_detect.roi_head import FasterRCNNOutputLayers3D\n",
    "from qct_3d_nod_detect.poolers import ROIPooler3D\n",
    "from qct_3d_nod_detect.anchor_generator_3d import DefaultAnchorGenerator3D\n",
    "from qct_3d_nod_detect.box_regression import Box3DTransform\n",
    "from qct_3d_nod_detect.matcher import Matcher\n",
    "from qct_3d_nod_detect.backbones import build_vit_backbone_with_fpn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38cfa0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-21 09:43:42.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqct_3d_nod_detect.backbones\u001b[0m:\u001b[36mload_state_dict_into_vit\u001b[0m:\u001b[36m351\u001b[0m - \u001b[1mMissing MAE keys during load: %s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: ['pos_embed'], Unexpected keys: []\n"
     ]
    }
   ],
   "source": [
    "anchor_generator_3d = DefaultAnchorGenerator3D(\n",
    "    sizes=[[1], [2], [4], [8]],\n",
    "    aspect_ratios_3d=[[(1.0, 1.0)], [(1.0, 1.0)], [(1.0, 1.0)], [(1.0, 1.0)]],\n",
    "    strides=[4, 8, 16, 32],\n",
    "    offset=0.5,\n",
    ")\n",
    "\n",
    "backbone_fpn = build_vit_backbone_with_fpn(\n",
    "    variant=\"L\",\n",
    "    ckpt_path=\"/raid15/utkarsh.singh/ctssl/experiments/mae_vit3d_l_grid384x512x512_cs128_ps8_randmask09/checkpoints/best.ckpt\",\n",
    "    scales=[1, 2, 0.5, 0.25],\n",
    "    out_channels=256\n",
    ")\n",
    "\n",
    "box3d2box3d_transform = Box3DTransform(\n",
    "    weights=(1.0, 1.0, 1.0, 1.0, 1.0, 1.0),\n",
    "    scale_clamp=math.log(1000.0),\n",
    ")\n",
    "\n",
    "rpn_head_3d = StandardRPNHead3d(\n",
    "    in_channels=256,\n",
    "    num_anchors=anchor_generator_3d.num_cell_anchors[0],\n",
    "    box_dim=6\n",
    ")\n",
    "\n",
    "anchor_matcher = Matcher(\n",
    "    thresholds=[0.3, 0.7],\n",
    "    labels=[0, -1, 1],\n",
    "    allow_low_quality_matches=True,\n",
    ")\n",
    "\n",
    "roi_pooler = ROIPooler3D(\n",
    "    output_size=(7, 7, 7),\n",
    "    canonical_level=4,\n",
    "    canonical_box_size=224,\n",
    "    pooler_type=\"ROIALign3DV2\",\n",
    "    scales=[1, 2, 0.5, 0.25]\n",
    ")\n",
    "\n",
    "rpn = RPN3D(\n",
    "    in_features=[\"p2\", \"p3\", \"p4\", \"p5\"],\n",
    "    head=rpn_head_3d,\n",
    "    anchor_generator=anchor_generator_3d,\n",
    "    anchor_matcher=anchor_matcher,\n",
    "    box3d_transform=box3d2box3d_transform,\n",
    "    batch_size_per_image=256,\n",
    "    positive_fraction=0.5,\n",
    "    pre_nms_topk=(200, 100),\n",
    "    post_nms_topk=(100, 50),\n",
    "    nms_thresh=0.5,\n",
    "    min_box_size=2.0,\n",
    "    box_reg_loss_type=\"smooth_l1\",\n",
    "    smooth_l1_beta=0.0,\n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "roi_head = FasterRCNNOutputLayers3D(\n",
    "    input_shape=(32, 256, 7, 7, 7),\n",
    "    num_classes=1,\n",
    "    box2box_transform=box3d2box3d_transform,\n",
    "    cls_agnostic_bbox_reg=False,\n",
    ")\n",
    "\n",
    "rcnn = GeneralizedRCNN3D(backbone_fpn=backbone_fpn,\n",
    "                         rpn=rpn,\n",
    "                         roi_pooler=roi_pooler,\n",
    "                         roi_head=roi_head,)\n",
    "\n",
    "lit_model = FasterRCNN3DLightning(\n",
    "    model=rcnn,\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ada94fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/users/ishan.tiwari/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA L40S') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type              | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | model | GeneralizedRCNN3D | 351 M  | train\n",
      "----------------------------------------------------\n",
      "347 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "351 M     Total params\n",
      "1,404.989 Total estimated model params size (MB)\n",
      "436       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636e1bcacaef45e895b214002b9b7300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "Cannot find field 'gt_classes' in the given Instances!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m trainer = pl.Trainer(\n\u001b[32m      2\u001b[39m     max_epochs=\u001b[32m5\u001b[39m,\n\u001b[32m      3\u001b[39m     accelerator=\u001b[33m\"\u001b[39m\u001b[33mgpu\u001b[39m\u001b[33m\"\u001b[39m,        \u001b[38;5;66;03m# or \"cpu\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m     enable_model_summary=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      9\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:560\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    558\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:49\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     52\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:598\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    591\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    592\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    593\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    594\u001b[39m     ckpt_path,\n\u001b[32m    595\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    596\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    597\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    601\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1011\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1008\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1016\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1055\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1053\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1054\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1056\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:458\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_epoch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    457\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:152\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m         \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:348\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_batch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    346\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    347\u001b[39m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m         batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    350\u001b[39m         batch_output = \u001b[38;5;28mself\u001b[39m.manual_optimization.run(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         closure()\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_ready()\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[32m    280\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_completed()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:177\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m pl_module._current_fx_name = hook_name\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    180\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1366\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimizer_step\u001b[39m(\n\u001b[32m   1336\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1337\u001b[39m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1340\u001b[39m     optimizer_closure: Optional[Callable[[], Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1341\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1342\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1343\u001b[39m \u001b[33;03m    the optimizer.\u001b[39;00m\n\u001b[32m   1344\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1364\u001b[39m \n\u001b[32m   1365\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1366\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/core/optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:123\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[32m    122\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/torch/optim/adamw.py:220\u001b[39m, in \u001b[36mAdamW.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n\u001b[32m    223\u001b[39m     params_with_grad: List[Tensor] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/plugins/precision/precision.py:109\u001b[39m, in \u001b[36mPrecision._wrap_closure\u001b[39m\u001b[34m(self, model, optimizer, closure)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap_closure\u001b[39m(\n\u001b[32m     97\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     98\u001b[39m     model: \u001b[33m\"\u001b[39m\u001b[33mpl.LightningModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     99\u001b[39m     optimizer: Steppable,\n\u001b[32m    100\u001b[39m     closure: Callable[[], Any],\n\u001b[32m    101\u001b[39m ) -> Any:\n\u001b[32m    102\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m    hook is called.\u001b[39;00m\n\u001b[32m    104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mself\u001b[39m._after_closure(model, optimizer)\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:131\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@torch\u001b[39m.enable_grad()\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> ClosureResult:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    134\u001b[39m         \u001b[38;5;28mself\u001b[39m.warning_cache.warn(\u001b[33m\"\u001b[39m\u001b[33m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/loops/optimization/automatic.py:319\u001b[39m, in \u001b[36m_AutomaticOptimization._training_step\u001b[39m\u001b[34m(self, kwargs)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[32m    309\u001b[39m \n\u001b[32m    310\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    315\u001b[39m \n\u001b[32m    316\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    317\u001b[39m trainer = \u001b[38;5;28mself\u001b[39m.trainer\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m training_step_output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.strategy.post_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer.world_size > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:329\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    332\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:391\u001b[39m, in \u001b[36mStrategy.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mtraining_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 110\u001b[39m, in \u001b[36mFasterRCNN3DLightning.training_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m    107\u001b[39m images = batch[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m]                \u001b[38;5;66;03m# Tensor[B, 1, D, H, W]\u001b[39;00m\n\u001b[32m    108\u001b[39m gt_instances = build_instances_3d(batch)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m losses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_instances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m total_loss = \u001b[38;5;28msum\u001b[39m(losses.values())\n\u001b[32m    114\u001b[39m \u001b[38;5;28mself\u001b[39m.log_dict(\n\u001b[32m    115\u001b[39m     {k: v.detach() \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m losses.items()},\n\u001b[32m    116\u001b[39m     prog_bar=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    117\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mGeneralizedRCNN3D.forward\u001b[39m\u001b[34m(self, images, gt_instances)\u001b[39m\n\u001b[32m     61\u001b[39m predictions = \u001b[38;5;28mself\u001b[39m.roi_head(roi_features)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     roi_losses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroi_head\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m     losses = {}\n\u001b[32m     68\u001b[39m     losses.update(rpn_losses)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Ishan_Nodseg/qct_3d_nod_detect/qct_3d_nod_detect/roi_head.py:273\u001b[39m, in \u001b[36mFasterRCNNOutputLayers3D.losses\u001b[39m\u001b[34m(self, predictions, proposals)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlosses\u001b[39m(\n\u001b[32m    266\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    267\u001b[39m         predictions,\n\u001b[32m    268\u001b[39m         proposals,\n\u001b[32m    269\u001b[39m ):\n\u001b[32m    271\u001b[39m     scores, proposal_deltas = predictions\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     gt_classes = torch.cat(\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgt_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m]\u001b[49m, dim=\u001b[32m0\u001b[39m)\n\u001b[32m    274\u001b[39m     proposal_boxes = torch.cat([p.proposal_boxes.tensor \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m proposals], dim=\u001b[32m0\u001b[39m)\n\u001b[32m    275\u001b[39m     gt_boxes = torch.cat([p.gt_boxes.tensor \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m proposals], dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Ishan_Nodseg/qct_3d_nod_detect/qct_3d_nod_detect/roi_head.py:273\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlosses\u001b[39m(\n\u001b[32m    266\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    267\u001b[39m         predictions,\n\u001b[32m    268\u001b[39m         proposals,\n\u001b[32m    269\u001b[39m ):\n\u001b[32m    271\u001b[39m     scores, proposal_deltas = predictions\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     gt_classes = torch.cat([\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgt_classes\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m proposals], dim=\u001b[32m0\u001b[39m)\n\u001b[32m    274\u001b[39m     proposal_boxes = torch.cat([p.proposal_boxes.tensor \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m proposals], dim=\u001b[32m0\u001b[39m)\n\u001b[32m    275\u001b[39m     gt_boxes = torch.cat([p.gt_boxes.tensor \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m proposals], dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Ishan_Nodseg/qct_3d_nod_detect/qct_3d_nod_detect/structures.py:219\u001b[39m, in \u001b[36mInstances3D.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Any:\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33m_fields\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fields:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot find field \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m in the given Instances!\u001b[39m\u001b[33m\"\u001b[39m.format(name))\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fields[name]\n",
      "\u001b[31mAttributeError\u001b[39m: Cannot find field 'gt_classes' in the given Instances!"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"gpu\",        # or \"cpu\"\n",
    "    devices=1,\n",
    "    precision=32,             # start without AMP\n",
    "    log_every_n_steps=5,\n",
    "    enable_checkpointing=False,\n",
    "    enable_model_summary=True,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    lit_model,\n",
    "    datamodule=datamodule\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c322431e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Instances3D(num_instances=32, image_height=128, image_width=128, fields=[proposal_boxes: Boxes3D(tensor([[ 81.6441,  57.4957, 113.2783,  82.4525,  58.4657, 114.3582],\n",
       "        [ 89.6444,  57.5030, 113.2813,  90.4595,  58.4642, 114.3541],\n",
       "        [ 81.6471,  65.5006, 113.2797,  82.4592,  66.4578, 114.3580],\n",
       "        [ 89.6488,  65.5047, 113.2798,  90.4632,  66.4579, 114.3544],\n",
       "        [ 33.6491,   9.5046,  65.2855,  34.4605,  10.4533,  66.3566],\n",
       "        [ 89.6460,  49.5050, 113.2894,  90.4539,  50.4666, 114.3578],\n",
       "        [ 81.6465,  49.4965, 113.2828,  82.4484,  50.4705, 114.3632],\n",
       "        [ 81.6515,  57.5023, 105.2821,  82.4588,  58.4588, 106.3612],\n",
       "        [ 73.6546,  57.4946, 113.2885,  74.4594,  58.4582, 114.3676],\n",
       "        [ 89.6516,  57.5062, 105.2839,  90.4614,  58.4574, 106.3589],\n",
       "        [ 11.8512,  11.9104,  42.9540,  19.2663,  18.9172,  51.5887],\n",
       "        [ 74.7293,  43.4593, 107.3351,  83.1400,  51.1693, 114.9526],\n",
       "        [ 75.2848,  75.7828, 107.5316,  83.4733,  83.9337, 115.3486],\n",
       "        [  5.7727,   5.9948,  21.6757,   9.8608,   9.5683,  25.7994],\n",
       "        [ 75.3507,  12.0206, 107.4445,  83.5686,  19.5281, 115.2450],\n",
       "        [ 11.6909,  12.0616,  75.0406,  19.4910,  19.1538,  83.7581],\n",
       "        [ 11.6461,  11.9662, 107.0859,  19.6413,  19.7614, 115.0759],\n",
       "        [ 43.2786,  43.5987, 106.9622,  51.2340,  51.5758, 114.6512],\n",
       "        [  6.0227,   6.1664, 101.7607,  10.0684,   9.7129, 105.9248],\n",
       "        [  5.8575,   6.0636,  53.7811,   9.9367,   9.6028,  57.9530],\n",
       "        [  5.8317,   6.0456,  69.7860,   9.9213,   9.6095,  73.9365],\n",
       "        [  5.8809,   6.0574,  37.7719,   9.9474,   9.6225,  41.9549],\n",
       "        [  5.8490,   6.0324,  85.7732,   9.9457,   9.6122,  89.9420],\n",
       "        [ 43.5908,  12.3322, 107.1647,  51.7525,  19.8431, 114.8174],\n",
       "        [  5.7556,   6.0182, 117.7253,  10.1330,   9.7772, 122.0121],\n",
       "        [ 43.4878,  12.4017,  75.0624,  51.3487,  18.9853,  82.8834],\n",
       "        [ 43.2206,  44.3498,  75.3336,  51.2423,  51.4558,  82.8754],\n",
       "        [ 69.7121,   6.1852, 101.8601,  73.7022,   9.6144, 106.3885],\n",
       "        [ 21.8619,   6.0852,  37.9557,  25.8285,   9.4845,  42.1670],\n",
       "        [ 53.7029,   6.1742, 101.8420,  57.6352,   9.6125, 106.3648],\n",
       "        [ 21.9515,   6.1907, 101.9236,  25.8616,   9.6091, 106.1923],\n",
       "        [ 21.8034,   6.0445,  85.9390,  25.7425,   9.5010,  90.1941]])), objectness_logits: tensor([0.2825, 0.2805, 0.2774, 0.2763, 0.2756, 0.2755, 0.2745, 0.2726, 0.2724,\n",
       "        0.2721, 0.1156, 0.1088, 0.1040, 0.0997, 0.0915, 0.0874, 0.0755, 0.0661,\n",
       "        0.0612, 0.0522, 0.0513, 0.0501, 0.0418, 0.0407, 0.0308, 0.0197, 0.0180,\n",
       "        0.0118, 0.0097, 0.0033, 0.0015, 0.0008])])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proposals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddbaa3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = batch[\"image\"]                # Tensor[B, 1, D, H, W]\n",
    "gt_instances = build_instances_3d(batch)\n",
    "features = backbone_fpn(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c97c5fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = build_image_list_3d(images)\n",
    "proposals, rpn_losses = rpn(\n",
    "                            image_list,\n",
    "                            features,\n",
    "                            gt_instances,\n",
    "                        )\n",
    "\n",
    "proposal_boxes = [x.proposal_boxes for x in proposals]\n",
    "# ----------------------------------\n",
    "# ROI Pooling\n",
    "# ----------------------------------\n",
    "x = list(features.values())\n",
    "roi_features = roi_pooler(\n",
    "    x,\n",
    "    proposal_boxes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6d63544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([62, 256, 7, 7, 7])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb918489",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Cannot find field 'gt_classes' in the given Instances!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m predictions = roi_head(roi_features)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     roi_losses = \u001b[43mroi_head\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     losses = {}\n\u001b[32m      7\u001b[39m     losses.update(rpn_losses)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Ishan_Nodseg/qct_3d_nod_detect/qct_3d_nod_detect/roi_head.py:273\u001b[39m, in \u001b[36mFasterRCNNOutputLayers3D.losses\u001b[39m\u001b[34m(self, predictions, proposals)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlosses\u001b[39m(\n\u001b[32m    266\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    267\u001b[39m         predictions,\n\u001b[32m    268\u001b[39m         proposals,\n\u001b[32m    269\u001b[39m ):\n\u001b[32m    271\u001b[39m     scores, proposal_deltas = predictions\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     gt_classes = torch.cat(\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgt_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m]\u001b[49m, dim=\u001b[32m0\u001b[39m)\n\u001b[32m    274\u001b[39m     proposal_boxes = torch.cat([p.proposal_boxes.tensor \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m proposals], dim=\u001b[32m0\u001b[39m)\n\u001b[32m    275\u001b[39m     gt_boxes = torch.cat([p.gt_boxes.tensor \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m proposals], dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Ishan_Nodseg/qct_3d_nod_detect/qct_3d_nod_detect/roi_head.py:273\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlosses\u001b[39m(\n\u001b[32m    266\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    267\u001b[39m         predictions,\n\u001b[32m    268\u001b[39m         proposals,\n\u001b[32m    269\u001b[39m ):\n\u001b[32m    271\u001b[39m     scores, proposal_deltas = predictions\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     gt_classes = torch.cat([\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgt_classes\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m proposals], dim=\u001b[32m0\u001b[39m)\n\u001b[32m    274\u001b[39m     proposal_boxes = torch.cat([p.proposal_boxes.tensor \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m proposals], dim=\u001b[32m0\u001b[39m)\n\u001b[32m    275\u001b[39m     gt_boxes = torch.cat([p.gt_boxes.tensor \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m proposals], dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Ishan_Nodseg/qct_3d_nod_detect/qct_3d_nod_detect/structures.py:219\u001b[39m, in \u001b[36mInstances3D.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Any:\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33m_fields\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fields:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot find field \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m in the given Instances!\u001b[39m\u001b[33m\"\u001b[39m.format(name))\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fields[name]\n",
      "\u001b[31mAttributeError\u001b[39m: Cannot find field 'gt_classes' in the given Instances!"
     ]
    }
   ],
   "source": [
    "predictions = roi_head(roi_features)\n",
    "\n",
    "if True:\n",
    "    roi_losses = roi_head.losses(predictions, proposals)\n",
    "\n",
    "    losses = {}\n",
    "    losses.update(rpn_losses)\n",
    "    losses.update(roi_losses)\n",
    "\n",
    "else:\n",
    "    detections, _ = self.roi_head.inference(predictions, proposals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad0c6fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([62, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2ccd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a455b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d23419f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qct_nod_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
