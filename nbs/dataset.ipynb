{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5cb85e",
   "metadata": {},
   "source": [
    "## Create toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a660574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3141994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sphere_volume(\n",
    "    shape=(64, 64, 64),\n",
    "    radius_range=(4, 8),\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        volume: (1, D, H, W)\n",
    "        box: (6,) -> (cx, cy, cz, dx, dy, dz)\n",
    "    \"\"\"\n",
    "    D, H, W = shape\n",
    "    volume = torch.zeros(1, D, H, W)\n",
    "\n",
    "    r = random.randint(*radius_range)\n",
    "    cx = random.randint(r, W - r - 1)\n",
    "    cy = random.randint(r, H - r - 1)\n",
    "    cz = random.randint(r, D - r - 1)\n",
    "\n",
    "    z, y, x = torch.meshgrid(\n",
    "        torch.arange(D),\n",
    "        torch.arange(H),\n",
    "        torch.arange(W),\n",
    "        indexing=\"ij\",\n",
    "    )\n",
    "\n",
    "    mask = (x - cx) ** 2 + (y - cy) ** 2 + (z - cz) ** 2 <= r ** 2\n",
    "    volume[0][mask] = 1.0\n",
    "\n",
    "    # Bounding box in (cx, cy, cz, dx, dy, dz)\n",
    "    x1, x2 = cx - r, cx + r\n",
    "    y1, y2 = cy - r, cy + r\n",
    "    z1, z2 = cz - r, cz + r\n",
    "\n",
    "    box = torch.tensor(\n",
    "        [\n",
    "            (x1 + x2) / 2,\n",
    "            (y1 + y2) / 2,\n",
    "            (z1 + z2) / 2,\n",
    "            x2 - x1,\n",
    "            y2 - y1,\n",
    "            z2 - z1,\n",
    "        ],\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    return volume, box\n",
    "\n",
    "def create_toy_detection_dataset(\n",
    "    root_dir,\n",
    "    num_train=500,\n",
    "    num_val=100,\n",
    "    shape=(64, 64, 64),\n",
    "):\n",
    "    root = Path(root_dir)\n",
    "\n",
    "    for split, n_samples in [(\"train\", num_train), (\"val\", num_val)]:\n",
    "        vol_dir = root / split / \"volumes\"\n",
    "        tgt_dir = root / split / \"targets\"\n",
    "        vol_dir.mkdir(parents=True, exist_ok=True)\n",
    "        tgt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            volume, box = generate_sphere_volume(shape)\n",
    "\n",
    "            torch.save(\n",
    "                volume,\n",
    "                vol_dir / f\"sample_{i:04d}.pt\",\n",
    "            )\n",
    "\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"boxes\": box.unsqueeze(0),      # (1, 6)\n",
    "                    \"labels\": torch.tensor([0]),    # single class\n",
    "                },\n",
    "                tgt_dir / f\"sample_{i:04d}.pt\",\n",
    "            )\n",
    "\n",
    "    print(f\"Dataset written to: {root}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e2b2149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset written to: /home/users/ishan.tiwari/Ishan_Nodseg/qct_3d_nod_detect/toy_dataset\n"
     ]
    }
   ],
   "source": [
    "create_toy_detection_dataset(\n",
    "    root_dir=\"/home/users/ishan.tiwari/Ishan_Nodseg/qct_3d_nod_detect/toy_dataset\",\n",
    "    num_train=1000,\n",
    "    num_val=200,\n",
    "    shape=(128,)*3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fff7faaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def visualize_3d_bbox(\n",
    "    root_dir,\n",
    "    split=\"train\",\n",
    "    sample_name=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Interactive visualization for a 3D volume with 3D bounding box.\n",
    "    \n",
    "    Expected structure:\n",
    "    root_dir/\n",
    "        train/\n",
    "            volumes/sample_xxxx.pt\n",
    "            targets/sample_xxxx.pt\n",
    "        val/\n",
    "            volumes/sample_xxxx.pt\n",
    "            targets/sample_xxxx.pt\n",
    "\n",
    "    target format:\n",
    "    {\n",
    "        'boxes': tensor([[x, y, z, dx, dy, dz]]),\n",
    "        'labels': tensor([class_id])\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    root_dir = Path(root_dir)\n",
    "    vol_dir = root_dir / split / \"volumes\"\n",
    "    tgt_dir = root_dir / split / \"targets\"\n",
    "\n",
    "    samples = sorted([p.stem for p in vol_dir.glob(\"*.pt\")])\n",
    "    if len(samples) == 0:\n",
    "        raise RuntimeError(\"No samples found\")\n",
    "\n",
    "    if sample_name is None:\n",
    "        sample_name = samples[0]\n",
    "\n",
    "    vol = torch.load(vol_dir / f\"{sample_name}.pt\")\n",
    "    tgt = torch.load(tgt_dir / f\"{sample_name}.pt\")\n",
    "\n",
    "    vol = vol.squeeze().cpu()\n",
    "    box = tgt[\"boxes\"][0].cpu()  # [x, y, z, dx, dy, dz]\n",
    "\n",
    "    x, y, z, dx, dy, dz = box\n",
    "    x0, x1 = int(x - dx / 2), int(x + dx / 2)\n",
    "    y0, y1 = int(y - dy / 2), int(y + dy / 2)\n",
    "    z0, z1 = int(z - dz / 2), int(z + dz / 2)\n",
    "\n",
    "    x0, y0, z0 = max(x0, 0), max(y0, 0), max(z0, 0)\n",
    "    x1, y1, z1 = min(x1, vol.shape[2]-1), min(y1, vol.shape[1]-1), min(z1, vol.shape[0]-1)\n",
    "\n",
    "    def plot_slice(axis, idx):\n",
    "        plt.figure(figsize=(5, 5))\n",
    "\n",
    "        if axis == \"z\":\n",
    "            img = vol[idx]\n",
    "            if z0 <= idx <= z1:\n",
    "                plt.gca().add_patch(\n",
    "                    plt.Rectangle(\n",
    "                        (x0, y0),\n",
    "                        x1 - x0,\n",
    "                        y1 - y0,\n",
    "                        fill=False,\n",
    "                        edgecolor=\"red\",\n",
    "                        linewidth=2,\n",
    "                    )\n",
    "                )\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "\n",
    "        elif axis == \"y\":\n",
    "            img = vol[:, idx, :]\n",
    "            if y0 <= idx <= y1:\n",
    "                plt.gca().add_patch(\n",
    "                    plt.Rectangle(\n",
    "                        (x0, z0),\n",
    "                        x1 - x0,\n",
    "                        z1 - z0,\n",
    "                        fill=False,\n",
    "                        edgecolor=\"red\",\n",
    "                        linewidth=2,\n",
    "                    )\n",
    "                )\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "\n",
    "        elif axis == \"x\":\n",
    "            img = vol[:, :, idx]\n",
    "            if x0 <= idx <= x1:\n",
    "                plt.gca().add_patch(\n",
    "                    plt.Rectangle(\n",
    "                        (y0, z0),\n",
    "                        y1 - y0,\n",
    "                        z1 - z0,\n",
    "                        fill=False,\n",
    "                        edgecolor=\"red\",\n",
    "                        linewidth=2,\n",
    "                    )\n",
    "                )\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "\n",
    "        plt.title(f\"{sample_name} | axis={axis} | slice={idx}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    axis_dd = widgets.Dropdown(\n",
    "        options=[\"z\", \"y\", \"x\"],\n",
    "        value=\"z\",\n",
    "        description=\"Axis:\",\n",
    "    )\n",
    "\n",
    "    slice_slider = widgets.IntSlider(\n",
    "        min=0,\n",
    "        max=vol.shape[0] - 1,\n",
    "        step=1,\n",
    "        value=int(z),\n",
    "        description=\"Slice:\",\n",
    "        continuous_update=False,\n",
    "    )\n",
    "\n",
    "    def update_slider(*args):\n",
    "        axis = axis_dd.value\n",
    "        if axis == \"z\":\n",
    "            slice_slider.max = vol.shape[0] - 1\n",
    "            slice_slider.value = int(z)\n",
    "        elif axis == \"y\":\n",
    "            slice_slider.max = vol.shape[1] - 1\n",
    "            slice_slider.value = int(y)\n",
    "        elif axis == \"x\":\n",
    "            slice_slider.max = vol.shape[2] - 1\n",
    "            slice_slider.value = int(x)\n",
    "\n",
    "    axis_dd.observe(update_slider, names=\"value\")\n",
    "\n",
    "    ui = widgets.VBox([axis_dd, slice_slider])\n",
    "    out = widgets.interactive_output(\n",
    "        plot_slice,\n",
    "        {\"axis\": axis_dd, \"idx\": slice_slider},\n",
    "    )\n",
    "\n",
    "    display(ui, out)\n",
    "\n",
    "\n",
    "# Example usage in notebook:\n",
    "# visualize_3d_bbox(\n",
    "#     root_dir=\"/path/to/dataset\",\n",
    "#     split=\"train\",\n",
    "#     sample_name=\"sample_0001\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc743681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0f6bcdd17e48c99230850bc86ab8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Axis:', options=('z', 'y', 'x'), value='z'), IntSlider(value=16, continuoâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8e8d9cc46848bd9ef54e5e7a1fb6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_3d_bbox(root_dir=\"/home/users/ishan.tiwari/Ishan_Nodseg/qct_3d_nod_detect/toy_dataset\", split=\"train\", sample_name=\"sample_0011\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab3864",
   "metadata": {},
   "source": [
    "## Load the toy sphere dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46295325",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from qct_3d_nod_detect.structures import Instances3D, Boxes3D, ImagesList3D\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "class GeneralizedRCNN3D(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            backbone: nn.Module,\n",
    "            rpn: nn.Module,\n",
    "            roi_heads: nn.Module,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.rpn = rpn\n",
    "        self.roi_heads = roi_heads\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            images: torch.Tensor,\n",
    "            targets: Optional[List[Instances3D]] = None,\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            training: dict of losses\n",
    "            inference: List[Instances3D]\n",
    "        \"\"\"\n",
    "\n",
    "        features: Dict[str, torch.Tensor] = self.backbone(images)\n",
    "        print(f\"Feature extraction completed, FPN meta - \")\n",
    "        for k, v in features.items():\n",
    "            print(f\"{k} - {v.shape}\")\n",
    "\n",
    "        image_list = ImagesList3D(\n",
    "                tensor=images,\n",
    "                image_sizes=[images.shape[-3:]] * images.shape[0],\n",
    "                )\n",
    "            \n",
    "        proposals, rpn_losses = self.rpn(\n",
    "            images=image_list,\n",
    "            features=features,\n",
    "            gt_instances=targets\n",
    "        )\n",
    "\n",
    "        roi_outputs = self.roi_heads(\n",
    "            features=features,\n",
    "            proposals=proposals,\n",
    "            targets=targets\n",
    "        )\n",
    "\n",
    "        if self.training:\n",
    "            losses = {}\n",
    "            losses.update(rpn_losses)\n",
    "            losses.update(roi_outputs)\n",
    "\n",
    "            return losses\n",
    "        \n",
    "        else:\n",
    "            return roi_outputs\n",
    "\n",
    "def build_targets(batch):\n",
    "    targets = []\n",
    "\n",
    "    B = len(batch[\"gt_boxes\"])\n",
    "    image_size = batch[\"image\"].shape[-3:]\n",
    "\n",
    "    for i in range(B):\n",
    "        inst = Instances3D(image_size=image_size)\n",
    "        inst.gt_boxes = Boxes3D(batch[\"gt_boxes\"][i])\n",
    "        inst.gt_classes = batch[\"gt_classes\"][i].long()\n",
    "        targets.append(inst)\n",
    "\n",
    "    return targets\n",
    "\n",
    "def build_image_list_3d(images: torch.Tensor) -> ImagesList3D:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        images: Tensor[B, C, D, H, W]\n",
    "    \"\"\"\n",
    "    image_sizes = [tuple(images.shape[-3:]) for _ in range(images.shape[0])]\n",
    "    return ImagesList3D(image_sizes)\n",
    "\n",
    "def build_instances_3d(batch):\n",
    "    instances = []\n",
    "\n",
    "    for boxes, classes in zip(batch[\"gt_boxes\"], batch[\"gt_classes\"]):\n",
    "        inst = Instances3D(image_size=batch[\"image\"].shape[-3:])\n",
    "        inst.gt_boxes = Boxes3D(boxes)\n",
    "        inst.gt_classes = classes\n",
    "        instances.append(inst)\n",
    "\n",
    "    return instances\n",
    "\n",
    "# Lightning module\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from typing import List\n",
    "from qct_3d_nod_detect.structures import Instances3D, Boxes3D, ImagesList3D\n",
    "from qct_3d_nod_detect.base_lightning import BaseLightningModule\n",
    "\n",
    "class FasterRCNN3DLightning(BaseLightningModule):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: nn.Module,\n",
    "            learning_rate: float = 1e-4,\n",
    "            grad_clip_val: float = 0.0,\n",
    "            grad_clip_algorithm: str = \"norm\",\n",
    "            log_on: str = \"step\"\n",
    "    ):\n",
    "\n",
    "        super().__init__(learning_rate=learning_rate)\n",
    "\n",
    "        self.model = model\n",
    "        self.grad_clip_val = grad_clip_val\n",
    "        self.grad_clip_algorithm = grad_clip_algorithm\n",
    "        self.log_on = log_on\n",
    "        \n",
    "    def _build_targets(\n",
    "        self,\n",
    "        batch\n",
    "    ):\n",
    "        \n",
    "        targets = []\n",
    "        image_size = batch['image'].shape[-3:] # (D, H, W)\n",
    "\n",
    "        for gt_boxes, gt_classes in zip(\n",
    "            batch['gt_boxes'], batch['gt_classes']\n",
    "        ):\n",
    "            \n",
    "            inst = Instances3D(image_size=image_size)\n",
    "            inst.gt_boxes = Boxes3D(gt_boxes)\n",
    "            inst.gt_classes = gt_classes.long()\n",
    "            targets.append(inst)\n",
    "\n",
    "        return targets\n",
    "    \n",
    "    def forward(\n",
    "            self,\n",
    "            images,\n",
    "            targets=None,\n",
    "    ):\n",
    "        \n",
    "        return self.model(images, targets)\n",
    "    \n",
    "    def training_step(\n",
    "            self, \n",
    "            batch, \n",
    "            batch_idx\n",
    "        ):\n",
    "\n",
    "        images = batch[\"image\"]\n",
    "        targets = self._build_targets(batch)\n",
    "\n",
    "        loss_dict: Dict[str, torch.Tensor] = self.model(images, targets)\n",
    "        total_loss = sum(loss_dict.values())\n",
    "\n",
    "        self.log_dict_helper(loss_dict, prefix=\"train/\")\n",
    "        \n",
    "        self.log(\n",
    "            \"train/loss_total\",\n",
    "            total_loss,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True\n",
    "        )\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "    def validation_step(\n",
    "            self, \n",
    "            batch, \n",
    "            batch_idx\n",
    "        ):\n",
    "\n",
    "        images = batch[\"image\"]\n",
    "        targets = self._build_targets(batch)\n",
    "\n",
    "        loss_dict = self.model(images, targets)\n",
    "        total_loss = sum(loss_dict.values())\n",
    "\n",
    "        self.log_dict_helper(loss_dict, prefix=\"val/\")\n",
    "        self.log(\n",
    "            \"val/loss_total\",\n",
    "            total_loss,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            weight_decay=1e-4,\n",
    "        )\n",
    "\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0ea0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "class ToySphereDetectionDataset(Dataset):\n",
    "    def __init__(self, root_dir, split=\"train\"):\n",
    "        self.root = Path(root_dir) / split\n",
    "        self.vol_dir = self.root / \"volumes\"\n",
    "        self.tgt_dir = self.root / \"targets\"\n",
    "\n",
    "        self.ids = sorted(p.stem for p in self.vol_dir.glob(\"*.pt\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sid = self.ids[idx]\n",
    "\n",
    "        volume = torch.load(self.vol_dir / f\"{sid}.pt\")   # (1, D, H, W)\n",
    "        target = torch.load(self.tgt_dir / f\"{sid}.pt\")\n",
    "\n",
    "        return {\n",
    "            \"image\": volume,                  # Tensor[1, D, H, W]\n",
    "            \"gt_boxes\": target[\"boxes\"],      # Tensor[N, 6]\n",
    "            \"gt_classes\": target[\"labels\"],   # Tensor[N]\n",
    "        }\n",
    "    \n",
    "class ToySphereDetectionDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: str,\n",
    "        batch_size: int = 2,\n",
    "        num_workers: int = 4,\n",
    "        pin_memory: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Called once per process\n",
    "        self.train_dataset = ToySphereDetectionDataset(\n",
    "            root_dir=self.root_dir,\n",
    "            split=\"train\",\n",
    "        )\n",
    "\n",
    "        self.val_dataset = ToySphereDetectionDataset(\n",
    "            root_dir=self.root_dir,\n",
    "            split=\"val\",\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=detection_collate,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            collate_fn=detection_collate,\n",
    "        )\n",
    "\n",
    "def detection_collate(batch):\n",
    "    return {\n",
    "        \"image\": torch.stack([b[\"image\"] for b in batch], dim=0),\n",
    "        \"gt_boxes\": [b[\"gt_boxes\"] for b in batch],\n",
    "        \"gt_classes\": [b[\"gt_classes\"] for b in batch],\n",
    "    }\n",
    "\n",
    "train_dataset = ToySphereDetectionDataset(\n",
    "    root_dir=\"/home/users/ishan.tiwari/Ishan_Nodseg/qct_3d_nod_detect/toy_dataset\",\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "val_dataset = ToySphereDetectionDataset(\n",
    "    root_dir=\"/home/users/ishan.tiwari/Ishan_Nodseg/qct_3d_nod_detect/toy_dataset\",\n",
    "    split=\"val\",\n",
    ")\n",
    "\n",
    "datamodule = ToySphereDetectionDataModule(root_dir='/home/users/ishan.tiwari/Ishan_Nodseg/qct_3d_nod_detect/toy_dataset',\n",
    "                                          batch_size=2,\n",
    "                                          num_workers=0,\n",
    "                                          pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de81fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,          # start small\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=detection_collate,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=detection_collate,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c89757d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 128, 128, 128])\n",
      "2\n",
      "torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "print(batch[\"image\"].shape)       # (B, 1, D, H, W)\n",
    "print(len(batch[\"gt_boxes\"]))     # B\n",
    "print(batch[\"gt_boxes\"][0].shape) # (N, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98831279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qct_3d_nod_detect.rpn import RPN3D, StandardRPNHead3d\n",
    "from qct_3d_nod_detect.faster_rcnn import FasterRCNNOutputLayers3D\n",
    "from qct_3d_nod_detect.poolers import ROIPooler3D\n",
    "from qct_3d_nod_detect.anchor_generator_3d import DefaultAnchorGenerator3D\n",
    "from qct_3d_nod_detect.box_regression import Box3DTransform\n",
    "from qct_3d_nod_detect.matcher import Matcher\n",
    "from qct_3d_nod_detect.roi_heads import ROIHeads3D\n",
    "from qct_3d_nod_detect.backbones import build_vit_backbone_with_fpn\n",
    "from qct_3d_nod_detect.box_heads import FastRCNNConvFCHead3D\n",
    "from qct_3d_nod_detect.layers import ShapeSpec\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38cfa0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-22 08:26:21.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mqct_3d_nod_detect.backbones\u001b[0m:\u001b[36mload_state_dict_into_vit\u001b[0m:\u001b[36m351\u001b[0m - \u001b[1mMissing MAE keys during load: %s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: ['pos_embed'], Unexpected keys: []\n"
     ]
    }
   ],
   "source": [
    "anchor_generator_3d = DefaultAnchorGenerator3D(\n",
    "    sizes=[[8], [16], [32], [64]],\n",
    "    aspect_ratios_3d=[[(1.0, 1.0)], [(1.0, 1.0)], [(1.0, 1.0)], [(1.0, 1.0)]],\n",
    "    strides=[4, 8, 16, 32],\n",
    "    offset=0.5,\n",
    ")\n",
    "\n",
    "backbone_fpn = build_vit_backbone_with_fpn(\n",
    "    variant=\"L\",\n",
    "    ckpt_path=\"/raid15/utkarsh.singh/ctssl/experiments/mae_vit3d_l_grid384x512x512_cs128_ps8_randmask09/checkpoints/best.ckpt\",\n",
    "    scales=[1, 2, 0.5, 0.25],\n",
    "    out_channels=256\n",
    ")\n",
    "\n",
    "box3d2box3d_transform = Box3DTransform(\n",
    "    weights=(1.0, 1.0, 1.0, 1.0, 1.0, 1.0),\n",
    "    scale_clamp=math.log(1000.0),\n",
    ")\n",
    "\n",
    "rpn_head_3d = StandardRPNHead3d(\n",
    "    in_channels=256,\n",
    "    num_anchors=anchor_generator_3d.num_cell_anchors[0],\n",
    "    box_dim=6\n",
    ")\n",
    "\n",
    "rpn_matcher = Matcher(\n",
    "    thresholds=[0.3, 0.7],\n",
    "    labels=[0, -1, 1],\n",
    "    allow_low_quality_matches=True,\n",
    ")\n",
    "\n",
    "roi_matcher = Matcher(\n",
    "    thresholds=[0.5],\n",
    "    labels=[0, 1],\n",
    "    allow_low_quality_matches=False,\n",
    ")\n",
    "\n",
    "roi_pooler = ROIPooler3D(\n",
    "    output_size=(7, 7, 7),\n",
    "    canonical_level=4,\n",
    "    canonical_box_size=224,\n",
    "    pooler_type=\"ROIALign3DV2\",\n",
    "    scales=[1, 2, 0.5, 0.25]\n",
    ")\n",
    "\n",
    "rpn = RPN3D(\n",
    "    in_features=[\"p2\", \"p3\", \"p4\", \"p5\"],\n",
    "    head=rpn_head_3d,\n",
    "    anchor_generator=anchor_generator_3d,\n",
    "    anchor_matcher=rpn_matcher,\n",
    "    box3d_transform=box3d2box3d_transform,\n",
    "    batch_size_per_image=256,\n",
    "    positive_fraction=0.5,\n",
    "    pre_nms_topk=(1000, 1000),\n",
    "    post_nms_topk=(600, 600),\n",
    "    nms_thresh=0.5,\n",
    "    min_box_size=2.0,\n",
    "    box_reg_loss_type=\"smooth_l1\",\n",
    "    smooth_l1_beta=0.0,\n",
    ")\n",
    "\n",
    "box_head = FastRCNNConvFCHead3D(\n",
    "    input_shape=ShapeSpec(256, 7, 7, 7),\n",
    "    conv_dims=[256,256],\n",
    "    fc_dims=[512]\n",
    ")\n",
    "\n",
    "output_layers = FasterRCNNOutputLayers3D(\n",
    "    input_dim=512,\n",
    "    num_classes=1,\n",
    "    box2box_transform=box3d2box3d_transform,\n",
    "    cls_agnostic_bbox_reg=False,\n",
    ")\n",
    "\n",
    "roi_head = ROIHeads3D(\n",
    "    num_classes=1,\n",
    "    batch_size_per_image=10,\n",
    "    positive_fraction=0.5,\n",
    "    proposal_matcher=roi_matcher,\n",
    "    roi_pooler=roi_pooler,\n",
    "    proposal_append_gt=True,\n",
    "    box_head=box_head,\n",
    "    box_predictor=output_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f8b2963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/ishan.tiwari/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:210: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
     ]
    }
   ],
   "source": [
    "model = GeneralizedRCNN3D(backbone=backbone_fpn, rpn=rpn, roi_heads=roi_head)\n",
    "lit_model = FasterRCNN3DLightning(model=model, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca10ca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction completed, FPN meta - \n",
      "p2 - torch.Size([2, 256, 32, 32, 32])\n",
      "p3 - torch.Size([2, 256, 16, 16, 16])\n",
      "p4 - torch.Size([2, 256, 8, 8, 8])\n",
      "p5 - torch.Size([2, 256, 4, 4, 4])\n",
      "Box features pool - torch.Size([20, 256, 7, 7, 7])\n",
      "Box features shape - torch.Size([20, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/ishan.tiwari/miniconda3/envs/qct_nod_seg/lib/python3.11/site-packages/lightning/pytorch/core/module.py:449: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    }
   ],
   "source": [
    "out = lit_model.training_step(batch=batch, batch_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c7030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ada94fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type              | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | model | GeneralizedRCNN3D | 399 M  | train\n",
      "----------------------------------------------------\n",
      "394 M     Trainable params\n",
      "4.2 M     Non-trainable params\n",
      "399 M     Total params\n",
      "1,596.187 Total estimated model params size (MB)\n",
      "445       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"gpu\",        # or \"cpu\"\n",
    "    devices=1,\n",
    "    precision=32,             # start without AMP\n",
    "    log_every_n_steps=5,\n",
    "    enable_checkpointing=False,\n",
    "    enable_model_summary=True,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    lit_model,\n",
    "    datamodule=datamodule\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d3c83a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qct_nod_seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
